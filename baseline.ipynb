{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import ast\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertTokenizerFast, BertModel, AdamW, TFBertModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import BertEmbeddings, BertSelfAttention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from apex import amp, optimizers\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "PRE_TRAINED = 'bert-base-uncased'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASPECT_NAMES = ['LEG', 'SIT', 'ENT', 'CUS', 'VOM', 'CLE', 'CKI', 'FNB']\n",
    "VOCAB_DIC = BertTokenizerFast.from_pretrained(PRE_TRAINED).get_vocab()\n",
    "TOPN = 50\n",
    "\n",
    "\n",
    "class BertBonz(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertBonz, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings.llr_embeddings = nn.ModuleList(nn.Embedding(4, 768, 3) for _ in range(len(ASPECT_NAMES)))\n",
    "        self.classifier = nn.Linear(768, config.num_aspect*3)\n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                llr_ids=None, \n",
    "                labels=None, \n",
    "                token_type_ids=None, \n",
    "                position_ids=None):\n",
    "        # BERT EMBEDDINGS NEW\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs_embeds = self.embeddings.word_embeddings(input_ids)\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        if llr_ids is not None:\n",
    "            temp = [self.embeddings.llr_embeddings[i](llr_ids[:,i,:]) for i in range(self.config.num_aspect)]\n",
    "            llr_embeddings = sum(temp)\n",
    "        else:\n",
    "            llr_embeddings = torch.zeros(inputs_embeds.size(), device=device).fill_(3).long()\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + llr_embeddings\n",
    "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.embeddings.dropout(embeddings)\n",
    "        \n",
    "        \n",
    "        # BERT ENCODER\n",
    "        encoder_outputs = self.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=None,\n",
    "            head_mask=[None]*12,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=self.config.output_attentions\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # CLASSIFIER\n",
    "        CLS_token = sequence_output[:,0]\n",
    "        predict = self.classifier(CLS_token)\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        if labels is not None:\n",
    "            loss = loss_fn(predict.view(input_shape[0], 3,-1), labels)\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), loss, CLS_token, sequence_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        else:\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), CLS_token, sequence_output) + encoder_outputs[1:]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def load_pretrained_weight(self):\n",
    "        sd = self.state_dict()\n",
    "        sd_bert_pretrained = BertModel.from_pretrained(PRE_TRAINED).state_dict()\n",
    "        for k in sd_bert_pretrained.keys():\n",
    "            if k in sd.keys():\n",
    "                sd[k] = sd_bert_pretrained[k]\n",
    "        self.load_state_dict(sd)\n",
    "        print('Succesfully load pre-trained weights')\n",
    "        \n",
    "    def llr_embed_pad(self):\n",
    "        for i in range(len(ASPECT_NAMES)):\n",
    "            temp = self.embeddings.llr_embeddings[i].weight.data\n",
    "            temp[-1,:] = torch.zeros(temp.size(1))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data, llr_words):\n",
    "        self.input_ids = torch.LongTensor(list(data.input_ids))\n",
    "        self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        if 'labels' in data.columns:\n",
    "            self.labels = torch.LongTensor(list(data.labels))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.llr_words = llr_words\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        tokens = self.data.input_ids[idx]\n",
    "        \n",
    "        llr_embedding = []\n",
    "        for aspect in ASPECT_NAMES:\n",
    "            temp = [3] * tokens.shape[0]\n",
    "            for j in range(tokens.shape[0]):\n",
    "                for class_, wordlist in llr_words[aspect].items():\n",
    "                    if tokens[j] in wordlist:\n",
    "                        temp[j] = class_\n",
    "                        break\n",
    "            llr_embedding.append(temp)\n",
    "        \n",
    "        llr_embedding = torch.stack([torch.LongTensor(i) for i in llr_embedding], 0)\n",
    "        \n",
    "        \n",
    "        outputs = (torch.LongTensor(tokens), llr_embedding)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            outputs = (torch.LongTensor(tokens), llr_embedding, torch.LongTensor(self.data.labels[idx]))\n",
    "        '''\n",
    "        if self.labels is None:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx])\n",
    "        else:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx], self.labels[idx])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    \n",
    "def split_aspect(data):\n",
    "    temp = np.full((8, data.shape[0]), 2, np.int)\n",
    "    for idx in range(data.shape[0]):\n",
    "        aspect = data[idx]\n",
    "        for i, asp in enumerate(['Legroom', 'Seat', 'Entertainment', 'Customer', 'Value', 'Cleanliness', 'Check-in', 'Food']):\n",
    "            for sub_asp in aspect:\n",
    "                if asp in sub_asp:\n",
    "                    pol = int(sub_asp[-1])\n",
    "                    temp[i, idx] = 1 if pol > 3 else 0\n",
    "                    break\n",
    "    return temp\n",
    "            \n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED)\n",
    "    input_ids = tokenizer(list(data))['input_ids']\n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, padding='post', truncating='post')\n",
    "    \n",
    "    return list(input_ids)\n",
    "    \n",
    "    \n",
    "def get_data():\n",
    "    col_names = ['TopNumber', 'AirlineName','ReviewerName','Rating','ReviewDate','ReviewTitle',\\\n",
    "                 'ReviewText','Tags', 'DateofTravel', 'Aspects', 'ResponserName', 'ResponseDate', 'ResponseText', 'ReviewerProfileUrl',\\\n",
    "                 'AirlineUrl','CrawlTime']\n",
    "    raw_data = pd.read_csv('./data/airline.txt', sep='\\t', header=None, names=col_names)\n",
    "    data = raw_data[['ReviewText', 'Rating', 'Aspects']]\n",
    "    data = data[data['Aspects'] != 'No filling in'].reset_index(drop=True) # Filter none aspects\n",
    "    data.Aspects = data.Aspects.str.split('|').values\n",
    "    \n",
    "    '''Split aspects to new columns'''\n",
    "    aspects_splitted = split_aspect(data.Aspects.values)\n",
    "    for i in range(len(ASPECT_NAMES)):\n",
    "        data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "        \n",
    "    data['input_ids'] = tokenize_data(data.ReviewText.values) # Generate input_ids from review text\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def word_class_freq(data, aspect_name, aspect_class=3):\n",
    "    temp = np.zeros((33000, aspect_class), np.int)\n",
    "    ids = data.input_ids.values\n",
    "    labels = data[aspect_name].values\n",
    "\n",
    "    for sub_ids, sub_lb in zip(ids, labels):\n",
    "        set_ids = set(sub_ids)\n",
    "        for ids in set_ids:\n",
    "            temp[ids, sub_lb] += 1\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def calculate_llr(temp_df, labels):\n",
    "    N = data.shape[0]\n",
    "    total_scores = []\n",
    "\n",
    "    for i in temp_df.index.values:\n",
    "        llr_scores = []\n",
    "        for class_ in [0,1,2]:\n",
    "            num_class_doc = np.sum(labels == class_)\n",
    "            n11 = temp_df.loc[i, class_]\n",
    "            n10 = num_class_doc - n11\n",
    "            n01 = temp_df.loc[i, 'total'] - n11\n",
    "            n00 = (N - n11 - n10 - n01)\n",
    "            pt = (1e-10 + n11 + n01)/N\n",
    "            p1 = n11/(1e-10 + n11 + n10)\n",
    "            p2 = n01/(1e-10 + n01 + n00)\n",
    "\n",
    "\n",
    "            try:\n",
    "                e1 = n11 * (math.log(pt) - math.log(p1))\n",
    "            except:\n",
    "                e1 = 0\n",
    "            try:\n",
    "                e2 = n10 * (math.log(1-pt) - math.log(1-p1))\n",
    "            except:\n",
    "                e2 = 0\n",
    "            try:\n",
    "                e3 = n01 * (math.log(pt) - math.log(p2))\n",
    "            except:\n",
    "                e3 = 0\n",
    "            try:\n",
    "                e4 = n00 * (math.log(1-pt) - math.log(1-p2))\n",
    "            except:\n",
    "                e4 = 0\n",
    "\n",
    "            llr_score = -2 * (e1+e2+e3+e4)\n",
    "            if n11 < n01:\n",
    "                llr_score = 0\n",
    "            llr_scores.append(llr_score)\n",
    "\n",
    "        total_scores.append(llr_scores)\n",
    "    \n",
    "    llr_df = pd.DataFrame(np.array(total_scores), index=temp_df.index, columns=temp_df.columns.values[:-1])\n",
    "\n",
    "    return llr_df\n",
    "\n",
    "\n",
    "def generate_llr_score(data, aspect):\n",
    "    temp = word_class_freq(data, aspect)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp)\n",
    "    temp_df['total'] = np.sum(temp, -1)\n",
    "    temp_df = temp_df[temp_df['total'] != 0]\n",
    "    temp_df = temp_df.drop(0,0)\n",
    "    \n",
    "    return calculate_llr(temp_df, data[aspect].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So, I had this trip aligned for family leisure...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2061, 1010, 1045, 2018, 2023, 4440, 1311...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Refund agreed to months ago but basically been...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:1, Seat comfort:1, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 25416, 8630, 3530, 2000, 2706, 3283, 202...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flying to London on Singapore Airlines, we had...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:3, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 3909, 2000, 2414, 2006, 5264, 7608, 1010...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought we were making a safe choice booking...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Customer service:1]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 1045, 2245, 2057, 2020, 2437, 1037, 3647...</td>\n",
       "      <td>[2, 2, 2, 0, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wonderful service on our trip out to New Zeala...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 6919, 2326, 2006, 2256, 4440, 2041, 2000...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141111</th>\n",
       "      <td>ANA is partnered with Air Canada for their fli...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 9617, 2003, 12404, 2007, 2250, 2710, 200...</td>\n",
       "      <td>[1, 1, 0, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141112</th>\n",
       "      <td>This is my first time flying with ANA. Overall...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2023, 2003, 2026, 2034, 2051, 3909, 2007...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141113</th>\n",
       "      <td>Excellent Airline to fly with, nice staff and ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6581, 8582, 2000, 4875, 2007, 1010, 3835...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141114</th>\n",
       "      <td>We traveled on ANA with our 1 year old baby. I...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 6158, 2006, 9617, 2007, 2256, 1015...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141115</th>\n",
       "      <td>Nice flight from Manila to Japan, the only dow...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:3, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 3835, 3462, 2013, 9011, 2000, 2900, 1010...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141116 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ReviewText  Rating  \\\n",
       "0       So, I had this trip aligned for family leisure...       5   \n",
       "1       Refund agreed to months ago but basically been...       1   \n",
       "2       Flying to London on Singapore Airlines, we had...       4   \n",
       "3       I thought we were making a safe choice booking...       1   \n",
       "4       Wonderful service on our trip out to New Zeala...       4   \n",
       "...                                                   ...     ...   \n",
       "141111  ANA is partnered with Air Canada for their fli...       4   \n",
       "141112  This is my first time flying with ANA. Overall...       5   \n",
       "141113  Excellent Airline to fly with, nice staff and ...       4   \n",
       "141114  We traveled on ANA with our 1 year old baby. I...       5   \n",
       "141115  Nice flight from Manila to Japan, the only dow...       4   \n",
       "\n",
       "                                                  Aspects  LEG  SIT  ENT  CUS  \\\n",
       "0       [Legroom:4, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "1       [Legroom:1, Seat comfort:1, In-flight Entertai...    0    0    0    0   \n",
       "2       [Legroom:3, Seat comfort:4, In-flight Entertai...    0    1    1    1   \n",
       "3                                    [Customer service:1]    2    2    2    0   \n",
       "4       [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "...                                                   ...  ...  ...  ...  ...   \n",
       "141111  [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    0    1   \n",
       "141112  [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "141113  [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "141114  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "141115  [Legroom:3, Seat comfort:4, In-flight Entertai...    0    1    1    1   \n",
       "\n",
       "        VOM  CLE  CKI  FNB                                          input_ids  \\\n",
       "0         1    1    1    1  [101, 2061, 1010, 1045, 2018, 2023, 4440, 1311...   \n",
       "1         0    0    0    0  [101, 25416, 8630, 3530, 2000, 2706, 3283, 202...   \n",
       "2         1    1    1    1  [101, 3909, 2000, 2414, 2006, 5264, 7608, 1010...   \n",
       "3         2    2    2    2  [101, 1045, 2245, 2057, 2020, 2437, 1037, 3647...   \n",
       "4         1    1    1    1  [101, 6919, 2326, 2006, 2256, 4440, 2041, 2000...   \n",
       "...     ...  ...  ...  ...                                                ...   \n",
       "141111    1    2    2    2  [101, 9617, 2003, 12404, 2007, 2250, 2710, 200...   \n",
       "141112    0    2    2    2  [101, 2023, 2003, 2026, 2034, 2051, 3909, 2007...   \n",
       "141113    1    2    2    2  [101, 6581, 8582, 2000, 4875, 2007, 1010, 3835...   \n",
       "141114    1    2    2    2  [101, 2057, 6158, 2006, 9617, 2007, 2256, 1015...   \n",
       "141115    1    2    2    2  [101, 3835, 3462, 2013, 9011, 2000, 2900, 1010...   \n",
       "\n",
       "                          labels  \n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2       [0, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3       [2, 2, 2, 0, 2, 2, 2, 2]  \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "...                          ...  \n",
       "141111  [1, 1, 0, 1, 1, 2, 2, 2]  \n",
       "141112  [1, 1, 1, 1, 0, 2, 2, 2]  \n",
       "141113  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "141114  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "141115  [0, 1, 1, 1, 1, 2, 2, 2]  \n",
       "\n",
       "[141116 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data()\n",
    "data['labels'] = list(data.iloc[:, 3:11].values)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data.ReviewText.values)\n",
    "labels = np.array(torch.tensor(data.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf7b4fc47344ab8b68b2fedd48f9556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = KFold(10)\n",
    "last_predict = []\n",
    "\n",
    "for train_idx, test_idx in tqdm.notebook.tqdm(kf.split(X)):\n",
    "    # Take train and test data\n",
    "    x_train = X[train_idx]\n",
    "    y_train = labels[train_idx]\n",
    "    x_test = X[test_idx]\n",
    "    y_test = labels[test_idx]\n",
    "    \n",
    "    # Initate model\n",
    "    clf = SVC()\n",
    "    multi_clf = MultiOutputClassifier(clf)\n",
    "    \n",
    "    multi_clf.fit(x_train, y_train)\n",
    "    predicted = multi_clf.predict(x_test).tolist()\n",
    "    last_predict.extend(predicted)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.06      0.11     38111\n",
      "           1       0.73      0.99      0.84    101257\n",
      "           2       0.00      0.00      0.00      1748\n",
      "\n",
      "    accuracy                           0.73    141116\n",
      "   macro avg       0.47      0.35      0.32    141116\n",
      "weighted avg       0.71      0.73      0.63    141116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.11      0.19     38710\n",
      "           1       0.74      0.99      0.84    100884\n",
      "           2       0.00      0.00      0.00      1522\n",
      "\n",
      "    accuracy                           0.74    141116\n",
      "   macro avg       0.48      0.37      0.35    141116\n",
      "weighted avg       0.72      0.74      0.66    141116\n",
      "\n",
      "ENT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.24      0.33     42524\n",
      "           1       0.64      0.95      0.76     82363\n",
      "           2       0.46      0.01      0.01     16229\n",
      "\n",
      "    accuracy                           0.63    141116\n",
      "   macro avg       0.55      0.40      0.37    141116\n",
      "weighted avg       0.59      0.63      0.55    141116\n",
      "\n",
      "CUS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.14      0.24     22629\n",
      "           1       0.85      1.00      0.91    116697\n",
      "           2       0.00      0.00      0.00      1790\n",
      "\n",
      "    accuracy                           0.85    141116\n",
      "   macro avg       0.57      0.38      0.39    141116\n",
      "weighted avg       0.84      0.85      0.80    141116\n",
      "\n",
      "VOM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.11      0.20     27884\n",
      "           1       0.79      0.99      0.88    109076\n",
      "           2       0.00      0.00      0.00      4156\n",
      "\n",
      "    accuracy                           0.79    141116\n",
      "   macro avg       0.53      0.37      0.36    141116\n",
      "weighted avg       0.77      0.79      0.72    141116\n",
      "\n",
      "CLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.00      0.00     14418\n",
      "           1       0.70      1.00      0.82     98994\n",
      "           2       0.65      0.00      0.00     27704\n",
      "\n",
      "    accuracy                           0.70    141116\n",
      "   macro avg       0.59      0.33      0.28    141116\n",
      "weighted avg       0.66      0.70      0.58    141116\n",
      "\n",
      "CKI:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.03      0.06     16145\n",
      "           1       0.69      1.00      0.82     97442\n",
      "           2       0.61      0.00      0.00     27529\n",
      "\n",
      "    accuracy                           0.69    141116\n",
      "   macro avg       0.63      0.34      0.29    141116\n",
      "weighted avg       0.67      0.69      0.57    141116\n",
      "\n",
      "FNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.22      0.31     36525\n",
      "           1       0.56      0.96      0.71     74041\n",
      "           2       0.48      0.00      0.00     30550\n",
      "\n",
      "    accuracy                           0.56    141116\n",
      "   macro avg       0.53      0.39      0.34    141116\n",
      "weighted avg       0.54      0.56      0.45    141116\n",
      "\n",
      "LEG:\t0.47\t0.35\t0.32\t0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIT:\t0.48\t0.37\t0.35\t0.74\n",
      "ENT:\t0.55\t0.40\t0.37\t0.63\n",
      "CUS:\t0.57\t0.38\t0.39\t0.85\n",
      "VOM:\t0.53\t0.37\t0.36\t0.79\n",
      "CLE:\t0.59\t0.33\t0.28\t0.70\n",
      "CKI:\t0.63\t0.34\t0.29\t0.69\n",
      "FNB:\t0.53\t0.39\t0.34\t0.56\n"
     ]
    }
   ],
   "source": [
    "y_true = labels\n",
    "y_predict = np.array(torch.tensor(last_predict))\n",
    "\n",
    "\n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}:\\n{classification_report(y_true[:,i], y_predict[:,i])}')\n",
    "    \n",
    "    \n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}:\\t{precision_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "{recall_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "{f1_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "{accuracy_score(y_true[:,i], y_predict[:,i]):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(y_predict), './result/RF.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141116, 63789)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
