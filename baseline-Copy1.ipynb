{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/udacity-pytorch-challengers/ideas-on-how-to-fine-tune-a-pre-trained-model-in-pytorch-184c47185a20\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertTokenizerFast, BertModel, AdamW, TFBertModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import BertEmbeddings, BertSelfAttention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from apex import amp, optimizers\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "COL_NAMES = ['TopNumber', 'AirlineName','ReviewerName','Rating','ReviewDate','ReviewTitle',\\\n",
    "             'ReviewText','Tags', 'DateofTravel', 'Aspects', 'ResponserName', 'ResponseDate', 'ResponseText', 'ReviewerProfileUrl',\\\n",
    "             'AirlineUrl','AirlineNation', 'CrawlTime']\n",
    "\n",
    "PRE_TRAINED = 'bert-base-uncased'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASPECT_NAMES = ['LEG', 'SIT', 'ENT', 'CUS', 'VOM', 'CLE', 'CKI', 'FNB']\n",
    "VOCAB_DIC = BertTokenizerFast.from_pretrained(PRE_TRAINED).get_vocab()\n",
    "TOPN = 50\n",
    "\n",
    "\n",
    "        \n",
    "# This one is implemented with weight loss per class            \n",
    "class BertBonzWeightLoss(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertBonzWeightLoss, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings.llr_embeddings = nn.ModuleList(nn.Embedding(4, 768, 3) for _ in range(len(ASPECT_NAMES)))\n",
    "        self.classifier = nn.Linear(768, config.num_aspect*3)\n",
    "        self.init_weights()\n",
    "        self.embeddings.llr_embeddings.apply(self._xavier)\n",
    "        self.pooler.apply(self._xavier)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                llr_ids=None, \n",
    "                labels=None, \n",
    "                token_type_ids=None, \n",
    "                position_ids=None,\n",
    "                weight_loss=None):\n",
    "        # BERT EMBEDDINGS NEW\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs_embeds = self.embeddings.word_embeddings(input_ids)\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        if llr_ids is not None:\n",
    "            temp = [self.embeddings.llr_embeddings[i](llr_ids[:,i,:]) for i in range(self.config.num_aspect)]\n",
    "            llr_embeddings = sum(temp)\n",
    "        else:\n",
    "            llr_embeddings = torch.zeros(inputs_embeds.size(), device=device).fill_(3).long()\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + llr_embeddings\n",
    "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.embeddings.dropout(embeddings)\n",
    "        \n",
    "        \n",
    "        # BERT ENCODER\n",
    "        encoder_outputs = self.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=None,\n",
    "            head_mask=[None]*12,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=self.config.output_attentions\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # CLASSIFIER\n",
    "        CLS_token = sequence_output[:,0]\n",
    "        predict = self.classifier(CLS_token)\n",
    "        \n",
    "        loss_fn = nn.functional.cross_entropy\n",
    "        if labels is not None:\n",
    "            if weight_loss is None:\n",
    "                loss = loss_fn(predict.view(input_shape[0], 3,-1), labels)\n",
    "            else:\n",
    "                loss = torch.tensor(0).float().to(DEVICE)\n",
    "                for asp_i in range(len(ASPECT_NAMES)):\n",
    "                    loss += loss_fn(predict.view(input_shape[0], 3,-1)[:,:,asp_i], labels[:,asp_i], weight_loss[asp_i, :])\n",
    "                loss /= len(ASPECT_NAMES)\n",
    "                    \n",
    "            outputs = (predict.view(input_shape[0], 3,-1), loss, CLS_token, sequence_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        else:\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), CLS_token, sequence_output) + encoder_outputs[1:]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def load_pretrained_weight(self):\n",
    "        sd = self.state_dict()\n",
    "        sd_bert_pretrained = BertModel.from_pretrained(PRE_TRAINED).state_dict()\n",
    "        for k in sd_bert_pretrained.keys():\n",
    "            if k in sd.keys():\n",
    "                sd[k] = sd_bert_pretrained[k]\n",
    "        self.load_state_dict(sd)\n",
    "        print('Succesfully load pre-trained weights')\n",
    "        \n",
    "    def llr_embed_pad(self):\n",
    "        for i in range(len(ASPECT_NAMES)):\n",
    "            temp = self.embeddings.llr_embeddings[i].weight.data\n",
    "            temp[-1,:] = torch.zeros(temp.size(1))\n",
    "            \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    "    def unfreeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "                \n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.embeddings.llr_embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "\n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data, llr_words):\n",
    "        self.input_ids = torch.LongTensor(list(data.input_ids))\n",
    "        self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        if 'labels' in data.columns:\n",
    "            self.labels = torch.LongTensor(list(data.labels))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.llr_words = llr_words\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        tokens = self.data.input_ids[idx]\n",
    "        \n",
    "        llr_embedding = []\n",
    "        for aspect in ASPECT_NAMES:\n",
    "            temp = [3] * tokens.shape[0]\n",
    "            for j in range(tokens.shape[0]):\n",
    "                for class_, wordlist in llr_words[aspect].items():\n",
    "                    if tokens[j] in wordlist:\n",
    "                        temp[j] = class_\n",
    "                        break\n",
    "            llr_embedding.append(temp)\n",
    "        \n",
    "        llr_embedding = torch.stack([torch.LongTensor(i) for i in llr_embedding], 0)\n",
    "        \n",
    "        \n",
    "        outputs = (torch.LongTensor(tokens), llr_embedding)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            outputs = (torch.LongTensor(tokens), llr_embedding, torch.LongTensor(self.data.labels[idx]))\n",
    "        '''\n",
    "        if self.labels is None:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx])\n",
    "        else:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx], self.labels[idx])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    \n",
    "def split_aspect(data):\n",
    "    temp = np.full((8, data.shape[0]), 2, np.int)\n",
    "    for idx in range(data.shape[0]):\n",
    "        aspect = data[idx]\n",
    "        for i, asp in enumerate(['Legroom', 'Seat', 'Entertainment', 'Customer', 'Value', 'Cleanliness', 'Check-in', 'Food']):\n",
    "            for sub_asp in aspect:\n",
    "                if asp in sub_asp:\n",
    "                    pol = int(sub_asp[-1])\n",
    "                    temp[i, idx] = 1 if pol > 3 else 0\n",
    "                    break\n",
    "    return temp\n",
    "            \n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED)\n",
    "    input_ids = tokenizer(list(data))['input_ids']\n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, padding='post', truncating='post')\n",
    "    \n",
    "    return (list(input_ids), tokenizer)\n",
    "    \n",
    "    \n",
    "def get_data(FILE_PATH, COL_NAMES):\n",
    "    raw_data = pd.read_csv(FILE_PATH, sep='\\t', header=None, names=COL_NAMES)\n",
    "    data = raw_data[['ReviewText', 'Rating', 'Aspects']]\n",
    "    data = data[data['Aspects'] != 'No filling in'] # Filter none aspects\n",
    "    data.Aspects = data.Aspects.str.split('|').values\n",
    "    \n",
    "    '''Split aspects to new columns'''\n",
    "    aspects_splitted = split_aspect(data.Aspects.values)\n",
    "    for i in range(len(ASPECT_NAMES)):\n",
    "        data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "        \n",
    "    data['input_ids'], tokenizer = tokenize_data(data.ReviewText.values) # Generate input_ids from review text\n",
    "    \n",
    "    return data, tokenizer\n",
    "\n",
    "\n",
    "def word_class_freq(data, aspect_name, aspect_class=3):\n",
    "    temp = np.zeros((33000, aspect_class), np.int)\n",
    "    ids = data.input_ids.values\n",
    "    labels = data[aspect_name].values\n",
    "\n",
    "    for sub_ids, sub_lb in zip(ids, labels):\n",
    "        set_ids = set(sub_ids)\n",
    "        for ids in set_ids:\n",
    "            temp[ids, sub_lb] += 1\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def calculate_llr(temp_df, labels):\n",
    "    N = data.shape[0]\n",
    "    total_scores = []\n",
    "\n",
    "    for i in temp_df.index.values:\n",
    "        llr_scores = []\n",
    "        for class_ in [0,1,2]:\n",
    "            num_class_doc = np.sum(labels == class_)\n",
    "            n11 = temp_df.loc[i, class_]\n",
    "            n10 = num_class_doc - n11\n",
    "            n01 = temp_df.loc[i, 'total'] - n11\n",
    "            n00 = (N - n11 - n10 - n01)\n",
    "            pt = (1e-10 + n11 + n01)/N\n",
    "            p1 = n11/(1e-10 + n11 + n10)\n",
    "            p2 = n01/(1e-10 + n01 + n00)\n",
    "\n",
    "\n",
    "            try:\n",
    "                e1 = n11 * (math.log(pt) - math.log(p1))\n",
    "            except:\n",
    "                e1 = 0\n",
    "            try:\n",
    "                e2 = n10 * (math.log(1-pt) - math.log(1-p1))\n",
    "            except:\n",
    "                e2 = 0\n",
    "            try:\n",
    "                e3 = n01 * (math.log(pt) - math.log(p2))\n",
    "            except:\n",
    "                e3 = 0\n",
    "            try:\n",
    "                e4 = n00 * (math.log(1-pt) - math.log(1-p2))\n",
    "            except:\n",
    "                e4 = 0\n",
    "\n",
    "            llr_score = -2 * (e1+e2+e3+e4)\n",
    "            if n11 < n01:\n",
    "                llr_score = 0\n",
    "            llr_scores.append(llr_score)\n",
    "\n",
    "        total_scores.append(llr_scores)\n",
    "    \n",
    "    llr_df = pd.DataFrame(np.array(total_scores), index=temp_df.index, columns=temp_df.columns.values[:-1])\n",
    "\n",
    "    return llr_df\n",
    "\n",
    "\n",
    "def generate_llr_score(data, aspect):\n",
    "    temp = word_class_freq(data, aspect)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp)\n",
    "    temp_df['total'] = np.sum(temp, -1)\n",
    "    temp_df = temp_df[temp_df['total'] != 0]\n",
    "    temp_df = temp_df.drop(0,0)\n",
    "    \n",
    "    return calculate_llr(temp_df, data[aspect].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7e9a5a52193c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_aspect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertBonzWeightLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_pretrained_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Load pre-trained BERT weights for BERT's layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-f0fc382cd76a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBertBonzWeightLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBertBonzWeightLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllr_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mASPECT_NAMES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tmuds\\documents\\github\\transformers\\src\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertPooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_input_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tmuds\\documents\\github\\transformers\\src\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;34m\"\"\" Initialize and prunes weights if needed. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;31m# Initialize weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;31m# Prune heads if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tmuds\\documents\\github\\transformers\\src\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36m_init_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertLayerNorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_aspect = 8\n",
    "model = BertBonzWeightLoss(config)\n",
    "\n",
    "model.load_pretrained_weight() # Load pre-trained BERT weights for BERT's layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "sum([i.numel() for i in model.parameters() if i.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model.embeddings.llr_embeddings.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, tokenizer = get_data('./data/data_new.txt', COL_NAMES)\n",
    "data['labels'] = list(data.iloc[:, 3:11].values)\n",
    "\n",
    "# Create weight\n",
    "weight_loss = []\n",
    "for aspect in ASPECT_NAMES:\n",
    "    temp = 1/data.loc[:, aspect].value_counts(0, 0).values\n",
    "    weight_loss.append(temp.tolist())\n",
    "    \n",
    "weight_loss = torch.tensor(weight_loss).to(DEVICE)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data.ReviewText.values)\n",
    "labels = np.array([i.tolist() for i in data.labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(10)\n",
    "\n",
    "method_predict = []\n",
    "\n",
    "for method in [DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier()]:\n",
    "    # TRAINING PHASE\n",
    "    last_predict = []\n",
    "    for train_idx, test_idx in tqdm.notebook.tqdm(kf.split(X)):\n",
    "        # Take train and test data\n",
    "        x_train = X[train_idx]\n",
    "        y_train = labels[train_idx]\n",
    "        x_test = X[test_idx]\n",
    "        y_test = labels[test_idx]\n",
    "\n",
    "        # Initate model\n",
    "        clf = method\n",
    "        multi_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "        multi_clf.fit(x_train, y_train)\n",
    "        predicted = multi_clf.predict(x_test).tolist()\n",
    "        last_predict.extend(predicted)\n",
    "    \n",
    "    # SAVE PREDICT\n",
    "    method_predict.append(torch.tensor(last_predict))\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    y_true = labels\n",
    "    y_predict = np.array(torch.tensor(last_predict))\n",
    "\n",
    "    for i, asp in enumerate(ASPECT_NAMES):\n",
    "        print(f'{asp}:\\t{precision_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "        {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "        {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\"):.2f}\\t\\\n",
    "        {accuracy_score(y_true[:,i], y_predict[:,i]):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(y_predict), './result/RF.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_predict in method_predict:    \n",
    "    for i, asp in enumerate(ASPECT_NAMES):\n",
    "        print(f'{asp}, {precision_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {accuracy_score(y_true[:,i], y_predict[:,i])*100:.2f}')\n",
    "    print(f'------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[-4, 0.1, 100], [-40, 10, 1]]\n",
    "b = [2,0]\n",
    "w_loss = [5,10,100]\n",
    "\n",
    "loss_fn = torch.nn.functional.cross_entropy\n",
    "\n",
    "# numpy\n",
    "print(((np.log(np.exp(a).sum(1)) - [a[i][b[i]] for i in range(2)])*[w_loss[b[i]] for i in range(2)]))\n",
    "\n",
    "# py torch\n",
    "print(loss_fn(torch.tensor(a).float(), torch.tensor(b), torch.tensor(w_loss).float(), reduction='none'))\n",
    "print(loss_fn(torch.tensor(a).float(), torch.tensor(b), reduction='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def focal_loss(labels, logits, alpha, gamma):\n",
    "    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n",
    "    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n",
    "    where pt is the probability of being classified to the true class.\n",
    "    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n",
    "    Args:\n",
    "      labels: A float tensor of size [batch, num_classes].\n",
    "      logits: A float tensor of size [batch, num_classes].\n",
    "      alpha: A float tensor of size [batch_size]\n",
    "        specifying per-example weight for balanced cross entropy.\n",
    "      gamma: A float scalar modulating loss from hard and easy examples.\n",
    "    Returns:\n",
    "      focal_loss: A float32 scalar representing normalized total loss.\n",
    "    \"\"\"    \n",
    "    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n",
    "\n",
    "    if gamma == 0.0:\n",
    "        modulator = 1.0\n",
    "    else:\n",
    "        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + \n",
    "            torch.exp(-1.0 * logits)))\n",
    "\n",
    "    loss = modulator * BCLoss\n",
    "\n",
    "    weighted_loss = alpha * loss\n",
    "    focal_loss = torch.sum(weighted_loss)\n",
    "\n",
    "    focal_loss /= torch.sum(labels)\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n",
    "    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n",
    "    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
    "    where Loss is one of the standard losses used for Neural Networks.\n",
    "    Args:\n",
    "      labels: A int tensor of size [batch].\n",
    "      logits: A float tensor of size [batch, no_of_classes].\n",
    "      samples_per_cls: A python list of size [no_of_classes].\n",
    "      no_of_classes: total number of classes. int\n",
    "      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n",
    "      beta: float. Hyperparameter for Class balanced loss.\n",
    "      gamma: float. Hyperparameter for Focal loss.\n",
    "    Returns:\n",
    "      cb_loss: A float tensor representing class balanced loss\n",
    "    \"\"\"\n",
    "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "    print(effective_num)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    print(weights)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "    print(weights)\n",
    "\n",
    "    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n",
    "\n",
    "    weights = torch.tensor(weights).float()\n",
    "    weights = weights.unsqueeze(0)\n",
    "    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n",
    "    weights = weights.sum(1)\n",
    "    weights = weights.unsqueeze(1)\n",
    "    weights = weights.repeat(1,no_of_classes)\n",
    "\n",
    "    if loss_type == \"focal\":\n",
    "        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n",
    "    elif loss_type == \"sigmoid\":\n",
    "        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)\n",
    "    elif loss_type == \"softmax\":\n",
    "        pred = logits.softmax(dim = 1)\n",
    "        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n",
    "    return cb_loss, weights\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    no_of_classes = 3\n",
    "    logits = torch.rand(10,no_of_classes).float()\n",
    "    labels = torch.randint(0,no_of_classes, size = (10,))\n",
    "    beta = 0.9999\n",
    "    gamma = 2.0\n",
    "    samples_per_cls = [10949,31161,757]\n",
    "    loss_type = \"softmax\"\n",
    "    cb_loss, weights = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n",
    "    print(labels)\n",
    "    print(cb_loss,'\\n', weights, weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_aspect = 8\n",
    "model = BertBonzWeightLoss(config)\n",
    "\n",
    "model.load_pretrained_weight() # Load pre-trained BERT weights for BERT's layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llr_embed_pad() # Set LLR embedding padding idx to 0-value tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.llr_embeddings.apply(model._xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.llr_embeddings[2].weight.data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.llr_embeddings[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_pretrained_weight()\n",
    "model.embeddings.word_embeddings.weight.data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.embeddings.llr_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
