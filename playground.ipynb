{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/udacity-pytorch-challengers/ideas-on-how-to-fine-tune-a-pre-trained-model-in-pytorch-184c47185a20\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertTokenizerFast, BertModel, AdamW, TFBertModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import BertEmbeddings, BertSelfAttention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from apex import amp, optimizers\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "COL_NAMES = ['TopNumber', 'AirlineName','ReviewerName','Rating','ReviewDate','ReviewTitle',\\\n",
    "             'ReviewText','Tags', 'DateofTravel', 'Aspects', 'ResponserName', 'ResponseDate', 'ResponseText', 'ReviewerProfileUrl',\\\n",
    "             'AirlineNation', 'CrawlTime']\n",
    "\n",
    "PRE_TRAINED = 'bert-base-uncased'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASPECT_NAMES = ['LEG', 'SIT', 'ENT', 'CUS', 'VOM', 'CLE', 'CKI', 'FNB']\n",
    "VOCAB_DIC = BertTokenizerFast.from_pretrained(PRE_TRAINED).get_vocab()\n",
    "TOPN = 50\n",
    "\n",
    "\n",
    "# This one is implemented with weight loss per class            \n",
    "class BertBonzWeightLoss(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertBonzWeightLoss, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings.llr_embeddings = nn.ModuleList(nn.Embedding(4, 768, 3) for _ in range(len(ASPECT_NAMES)))\n",
    "        self.classifier = nn.Linear(768, config.num_aspect*3)\n",
    "        self.init_weights()\n",
    "        self.embeddings.llr_embeddings.apply(self._xavier)\n",
    "        self.pooler.apply(self._xavier)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                llr_ids=None, \n",
    "                labels=None, \n",
    "                token_type_ids=None, \n",
    "                position_ids=None,\n",
    "                weight_loss=None):\n",
    "        # BERT EMBEDDINGS NEW\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs_embeds = self.embeddings.word_embeddings(input_ids)\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        if llr_ids is not None:\n",
    "            temp = [self.embeddings.llr_embeddings[i](llr_ids[:,i,:]) for i in range(self.config.num_aspect)]\n",
    "            llr_embeddings = sum(temp)\n",
    "        else:\n",
    "            llr_embeddings = torch.zeros(inputs_embeds.size(), device=device).fill_(3).long()\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + llr_embeddings\n",
    "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.embeddings.dropout(embeddings)\n",
    "        \n",
    "        \n",
    "        # BERT ENCODER\n",
    "        encoder_outputs = self.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=None,\n",
    "            head_mask=[None]*12,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=self.config.output_attentions\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # CLASSIFIER\n",
    "        CLS_token = sequence_output[:,0]\n",
    "        predict = self.classifier(CLS_token)\n",
    "        \n",
    "        loss_fn = nn.functional.cross_entropy\n",
    "        if labels is not None:\n",
    "            if weight_loss is None:\n",
    "                loss = loss_fn(predict.view(input_shape[0], 3,-1), labels)\n",
    "            else:\n",
    "                loss = torch.tensor(0).float().to(DEVICE)\n",
    "                for asp_i in range(len(ASPECT_NAMES)):\n",
    "                    loss += loss_fn(predict.view(input_shape[0], 3,-1)[:,:,asp_i], labels[:,asp_i], weight_loss[asp_i, :])\n",
    "                loss /= len(ASPECT_NAMES)\n",
    "                    \n",
    "            outputs = (predict.view(input_shape[0], 3,-1), loss, CLS_token, sequence_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        else:\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), CLS_token, sequence_output) + encoder_outputs[1:]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def load_pretrained_weight(self):\n",
    "        sd = self.state_dict()\n",
    "        sd_bert_pretrained = BertModel.from_pretrained(PRE_TRAINED).state_dict()\n",
    "        for k in sd_bert_pretrained.keys():\n",
    "            if k in sd.keys():\n",
    "                sd[k] = sd_bert_pretrained[k]\n",
    "        self.load_state_dict(sd)\n",
    "        print('Succesfully load pre-trained weights')\n",
    "        \n",
    "    def llr_embed_pad(self):\n",
    "        for i in range(len(self.embeddings.llr_embeddings)):\n",
    "            temp = self.embeddings.llr_embeddings[i].weight.data\n",
    "            temp[-1,:] = torch.zeros(temp.size(1))\n",
    "        \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    "    def unfreeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "                \n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.embeddings.llr_embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True            \n",
    "    \n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data, llr_words):\n",
    "        self.input_ids = torch.LongTensor(list(data.input_ids))\n",
    "        self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        if 'labels' in data.columns:\n",
    "            self.labels = torch.LongTensor(list(data.labels))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.llr_words = llr_words\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        tokens = self.data.input_ids[idx]\n",
    "        \n",
    "        llr_embedding = []\n",
    "        for aspect in ASPECT_NAMES:\n",
    "            temp = [3] * tokens.shape[0]\n",
    "            for j in range(tokens.shape[0]):\n",
    "                for class_, wordlist in llr_words[aspect].items():\n",
    "                    if tokens[j] in wordlist:\n",
    "                        temp[j] = class_\n",
    "                        break\n",
    "            llr_embedding.append(temp)\n",
    "        \n",
    "        llr_embedding = torch.stack([torch.LongTensor(i) for i in llr_embedding], 0)\n",
    "        \n",
    "        \n",
    "        outputs = (torch.LongTensor(tokens), llr_embedding)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            outputs = (torch.LongTensor(tokens), llr_embedding, torch.LongTensor(self.data.labels[idx]))\n",
    "        '''\n",
    "        if self.labels is None:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx])\n",
    "        else:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx], self.labels[idx])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    \n",
    "def split_aspect(data):\n",
    "    temp = np.full((8, data.shape[0]), 2, np.int)\n",
    "    for idx in range(data.shape[0]):\n",
    "        aspect = data[idx]\n",
    "        for i, asp in enumerate(['Legroom', 'Seat', 'Entertainment', 'Customer', 'Value', 'Cleanliness', 'Check-in', 'Food']):\n",
    "            for sub_asp in aspect:\n",
    "                if asp in sub_asp:\n",
    "                    pol = int(sub_asp[-1])\n",
    "                    temp[i, idx] = 1 if pol > 3 else 0\n",
    "                    break\n",
    "    return temp\n",
    "            \n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED)\n",
    "    input_ids = tokenizer(list(data))['input_ids']\n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, padding='post', truncating='post')\n",
    "    \n",
    "    return (list(input_ids), tokenizer)\n",
    "    \n",
    "    \n",
    "def get_data(FILE_PATH, COL_NAMES):\n",
    "    raw_data = pd.read_csv(FILE_PATH, sep='\\t', header=None, names=COL_NAMES)\n",
    "    data = raw_data[['ReviewText', 'Rating', 'Aspects']]\n",
    "    data = data[data['Aspects'] != 'No filling in'] # Filter none aspects\n",
    "    data.Aspects = data.Aspects.str.split('|').values\n",
    "    \n",
    "    '''Split aspects to new columns'''\n",
    "    aspects_splitted = split_aspect(data.Aspects.values)\n",
    "    for i in range(len(ASPECT_NAMES)):\n",
    "        data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "        \n",
    "    data['input_ids'], tokenizer = tokenize_data(data.ReviewText.values) # Generate input_ids from review text\n",
    "    \n",
    "    return data, tokenizer\n",
    "\n",
    "\n",
    "def word_class_freq(data, aspect_name, aspect_class=3):\n",
    "    temp = np.zeros((33000, aspect_class), np.int)\n",
    "    ids = data.input_ids.values\n",
    "    labels = data[aspect_name].values\n",
    "\n",
    "    for sub_ids, sub_lb in zip(ids, labels):\n",
    "        set_ids = set(sub_ids)\n",
    "        for ids in set_ids:\n",
    "            temp[ids, sub_lb] += 1\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def calculate_llr(temp_df, labels):\n",
    "    N = data.shape[0]\n",
    "    total_scores = []\n",
    "\n",
    "    for i in temp_df.index.values:\n",
    "        llr_scores = []\n",
    "        for class_ in [0,1,2]:\n",
    "            num_class_doc = np.sum(labels == class_)\n",
    "            n11 = temp_df.loc[i, class_]\n",
    "            n10 = num_class_doc - n11\n",
    "            n01 = temp_df.loc[i, 'total'] - n11\n",
    "            n00 = (N - n11 - n10 - n01)\n",
    "            pt = (1e-10 + n11 + n01)/N\n",
    "            p1 = n11/(1e-10 + n11 + n10)\n",
    "            p2 = n01/(1e-10 + n01 + n00)\n",
    "\n",
    "\n",
    "            try:\n",
    "                e1 = n11 * (math.log(pt) - math.log(p1))\n",
    "            except:\n",
    "                e1 = 0\n",
    "            try:\n",
    "                e2 = n10 * (math.log(1-pt) - math.log(1-p1))\n",
    "            except:\n",
    "                e2 = 0\n",
    "            try:\n",
    "                e3 = n01 * (math.log(pt) - math.log(p2))\n",
    "            except:\n",
    "                e3 = 0\n",
    "            try:\n",
    "                e4 = n00 * (math.log(1-pt) - math.log(1-p2))\n",
    "            except:\n",
    "                e4 = 0\n",
    "\n",
    "            llr_score = -2 * (e1+e2+e3+e4)\n",
    "            if n11 < n01:\n",
    "                llr_score = 0\n",
    "            llr_scores.append(llr_score)\n",
    "\n",
    "        total_scores.append(llr_scores)\n",
    "    \n",
    "    llr_df = pd.DataFrame(np.array(total_scores), index=temp_df.index, columns=temp_df.columns.values[:-1])\n",
    "\n",
    "    return llr_df\n",
    "\n",
    "\n",
    "def generate_llr_score(data, aspect):\n",
    "    temp = word_class_freq(data, aspect)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp)\n",
    "    temp_df['total'] = np.sum(temp, -1)\n",
    "    temp_df = temp_df[temp_df['total'] != 0]\n",
    "    temp_df = temp_df.drop(0,0)\n",
    "    \n",
    "    return calculate_llr(temp_df, data[aspect].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PRE-PROCESSED DATA (IF ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ecc20f42f945528bd0d5cc47a98b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90b7fc989684335a194038cb917c172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42867.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1030fb1c3d4428a07e3a8b50570072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42867.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8666ddf26e42ca9c12a1895cf26340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42867.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0924, 0.0643, 0.8433],\n",
       "        [0.0776, 0.0554, 0.8670],\n",
       "        [0.2724, 0.2198, 0.5078],\n",
       "        [0.1024, 0.0561, 0.8415],\n",
       "        [0.1517, 0.0956, 0.7527],\n",
       "        [0.4734, 0.1872, 0.3394],\n",
       "        [0.4486, 0.1954, 0.3559],\n",
       "        [0.3372, 0.2571, 0.4057]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load n process read data\n",
    "data = pd.read_csv('./data/pre-processed_50.csv', sep='\\t', index_col=0)\n",
    "\n",
    "for col in tqdm.notebook.tqdm(['input_ids', 'labels', 'llr_embeddings']):\n",
    "    data[col] = [ast.literal_eval(i) for i in tqdm.notebook.tqdm(data[col].values)]\n",
    "\n",
    "\n",
    "# CALCULATE WEIGHT LOSS\n",
    "labels = pd.DataFrame([i for i in data.labels])\n",
    "beta = 0.9999\n",
    "#beta = (data.shape[0] - 1) / data.shape[0]\n",
    "weight_loss = []\n",
    "\n",
    "for i in labels:\n",
    "    n_sample = labels.loc[:, i].value_counts(0, 0).values\n",
    "    n_sample = 1.0 - np.power(beta, n_sample)\n",
    "    n_sample = (1.0 - beta) / n_sample\n",
    "    n_sample = n_sample / np.sum(n_sample)\n",
    "    weight_loss.append(n_sample)\n",
    "\n",
    "weight_loss = torch.tensor(weight_loss, device=DEVICE).float()\n",
    "weight_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With everyone trying to get home in the Covid ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ad a lot of people did, we had to scramble to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After coming into Changi airport and worrying ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>We booked to fly from Heathrow to Newark. The ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:2, Seat comfort:2, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>Love Virgin, great staff, food good, quality o...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>Virgin upper class is outstanding, really very...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>Virgins premium economy is the best I have com...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:3, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>At my age I cannot face an overnight in econom...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152574 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ReviewText  Rating  \\\n",
       "0       With everyone trying to get home in the Covid ...       4   \n",
       "1       Ad a lot of people did, we had to scramble to ...       5   \n",
       "2       After coming into Changi airport and worrying ...       4   \n",
       "3       Great service, great plane, great pricing. We ...       4   \n",
       "4       My husband and I were to fly home from Houston...       1   \n",
       "...                                                   ...     ...   \n",
       "190981  We booked to fly from Heathrow to Newark. The ...       1   \n",
       "190982  Love Virgin, great staff, food good, quality o...       5   \n",
       "190983  Virgin upper class is outstanding, really very...       5   \n",
       "190984  Virgins premium economy is the best I have com...       5   \n",
       "190985  At my age I cannot face an overnight in econom...       2   \n",
       "\n",
       "                                                  Aspects  LEG  SIT  ENT  CUS  \\\n",
       "0       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "1       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "2       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "3       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "4       [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    0   \n",
       "...                                                   ...  ...  ...  ...  ...   \n",
       "190981  [Legroom:2, Seat comfort:2, In-flight Entertai...    0    0    0    0   \n",
       "190982  [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "190983  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "190984  [Legroom:3, Seat comfort:5, In-flight Entertai...    0    1    1    0   \n",
       "190985  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    0    0   \n",
       "\n",
       "        VOM  CLE  CKI  FNB                                          input_ids  \\\n",
       "0         1    1    1    1  [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...   \n",
       "1         1    1    1    1  [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...   \n",
       "2         1    1    1    1  [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...   \n",
       "3         1    1    1    1  [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...   \n",
       "4         1    1    1    1  [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...   \n",
       "...     ...  ...  ...  ...                                                ...   \n",
       "190981    0    2    2    2  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...   \n",
       "190982    1    2    2    2  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...   \n",
       "190983    1    2    2    2  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...   \n",
       "190984    0    2    2    2  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...   \n",
       "190985    0    2    2    2  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...   \n",
       "\n",
       "                          labels  \n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4       [1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "...                          ...  \n",
       "190981  [0, 0, 0, 0, 0, 2, 2, 2]  \n",
       "190982  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190983  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190984  [0, 1, 1, 0, 0, 2, 2, 2]  \n",
       "190985  [1, 1, 0, 0, 0, 2, 2, 2]  \n",
       "\n",
       "[152574 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, tokenizer = get_data('./data/data_v3.txt', COL_NAMES)\n",
    "data['labels'] = list(data.iloc[:, 3:11].values)\n",
    "\n",
    "# CALCULATE WEIGHT LOSS\n",
    "labels = pd.DataFrame([i for i in data.labels])\n",
    "beta = 0.9999\n",
    "#beta = (data.shape[0] - 1) / data.shape[0]\n",
    "weight_loss = []\n",
    "\n",
    "for i in labels:\n",
    "    n_sample = labels.loc[:, i].value_counts(0, 0).values\n",
    "    n_sample = 1.0 - np.power(beta, n_sample)\n",
    "    n_sample = (1.0 - beta) / n_sample\n",
    "    n_sample = n_sample / np.sum(n_sample)\n",
    "    weight_loss.append(n_sample)\n",
    "\n",
    "weight_loss = torch.tensor(weight_loss, device=DEVICE).float()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE LLR SCORES & WORDLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1ac06f77664bebb3abd8f8ef245750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Calculate LLR scores', max=8.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8227c70e5d3f4348b67b3f9c331c0f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate top LLR words', max=8.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9adad2cab44cd9a7a46b1af329d576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=152574.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stopwords in English\n",
    "stopwords_ids = tokenizer.convert_tokens_to_ids(stopwords.words('english'))\n",
    "\n",
    "llr_scores = {}\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Calculate LLR scores'):\n",
    "    llr_df = generate_llr_score(data, aspect)\n",
    "    \n",
    "    # Clear stopword ids\n",
    "    llr_df = llr_df.drop(stopwords_ids, 0)\n",
    "    \n",
    "    llr_scores[aspect] = llr_df\n",
    "\n",
    "\n",
    "llr_words = dict()\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Generate top LLR words'):\n",
    "    kw_label = dict()\n",
    "    for class_ in [0,1,2]:\n",
    "        # Sort keywords based on aspect, class and top_n words\n",
    "        kw_list = list(llr_scores[aspect][class_].sort_values(ascending=False)[:TOPN].index)\n",
    "        \n",
    "        kw_label[class_] = kw_list\n",
    "        \n",
    "    llr_words[aspect] = kw_label\n",
    "\n",
    "llr_embedding_list = []\n",
    "\n",
    "for idx in tqdm.notebook.tqdm(data.index):\n",
    "    tokens = data.input_ids[idx]\n",
    "    \n",
    "    llr_embedding = []\n",
    "    for aspect in ASPECT_NAMES:\n",
    "        temp = [3] * tokens.shape[0]\n",
    "        for j in range(tokens.shape[0]):\n",
    "            for class_, wordlist in llr_words[aspect].items():\n",
    "                if tokens[j] in wordlist:\n",
    "                    temp[j] = class_\n",
    "                    break\n",
    "        llr_embedding.append(temp)\n",
    "    \n",
    "    llr_embedding_list.append(llr_embedding)\n",
    "\n",
    "#data['llr_embeddings'] = [[[0]*512]*8] * data.shape[0]\n",
    "data['llr_embeddings'] = llr_embedding_list\n",
    "\n",
    "# Turn numpy array to list to store easier\n",
    "for i in data.keys()[-3:]:\n",
    "    data[i] = data[i].map(list)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,-3:].to_csv('./data/pre-processed_50_v3.csv', sep='\\t')\n",
    "data.iloc[:1000,-3:].to_csv('./data/sample_50_v3.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_aspect = len(ASPECT_NAMES)\n",
    "model = BertBonzWeightLoss(config)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.load_pretrained_weight() # Load pre-trained BERT weights for BERT's layers \n",
    "model.llr_embed_pad() # Set LLR embedding padding idx to 0-value tensor\n",
    "\n",
    "\n",
    "\n",
    "''' Using apex for faster training\n",
    "optimizer_list = []\n",
    "for i in range(10):\n",
    "    optimizer_list.append(AdamW(model.parameters(), lr=3e-5, correct_bias=False))\n",
    "\n",
    "model = amp.initialize(model, opt_level=\"O2\", verbosity=0)\n",
    "''' \n",
    "\n",
    "''' Save origin state dict of Model and Optimizer'''\n",
    "torch.save(model.state_dict(), 'origin_sd.pth')\n",
    "origin_sd = torch.load('origin_sd.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n",
    "_, optimizer = amp.initialize([], optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Learning-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59773697856f4539b3493f813d502b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 6244.56, Learning Rate = 2.00e-06\n",
      "Epoch: 1, Loss = 5926.94, Learning Rate = 4.00e-06\n",
      "Epoch: 2, Loss = 5830.10, Learning Rate = 6.00e-06\n",
      "Epoch: 3, Loss = 5766.99, Learning Rate = 8.00e-06\n",
      "Epoch: 4, Loss = 5761.07, Learning Rate = 1.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea09d2e607e445128b2f1c6915cb7983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 5547.02, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 5400.37, Learning Rate = 4.00e-05\n",
      "Epoch: 2, Loss = 5379.64, Learning Rate = 6.00e-05\n",
      "Epoch: 3, Loss = 5304.28, Learning Rate = 8.00e-05\n",
      "Epoch: 4, Loss = 5252.24, Learning Rate = 1.00e-04\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c5041b4441441eb948a79b5be996c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 5216.68, Learning Rate = 2.00e-04\n",
      "Epoch: 1, Loss = 5138.04, Learning Rate = 4.00e-04\n",
      "Epoch: 2, Loss = 5089.22, Learning Rate = 6.00e-04\n",
      "Epoch: 3, Loss = 5098.34, Learning Rate = 8.00e-04\n",
      "Epoch: 4, Loss = 4983.20, Learning Rate = 1.00e-03\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905b1498114d488e947a06d1d5157bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 5140.37, Learning Rate = 2.00e-03\n",
      "Epoch: 1, Loss = 5198.16, Learning Rate = 4.00e-03\n",
      "Epoch: 2, Loss = 5744.86, Learning Rate = 6.00e-03\n",
      "Epoch: 3, Loss = 5603.75, Learning Rate = 8.00e-03\n",
      "Epoch: 4, Loss = 5859.97, Learning Rate = 1.00e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d461ebe17b9c4996944b31bd64ff4e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 7149.28, Learning Rate = 2.00e-02\n",
      "Epoch: 1, Loss = 9230.91, Learning Rate = 4.00e-02\n",
      "Epoch: 2, Loss = 11248.39, Learning Rate = 6.00e-02\n",
      "Epoch: 3, Loss = 16239.48, Learning Rate = 8.00e-02\n",
      "Epoch: 4, Loss = 21634.04, Learning Rate = 1.00e-01\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6334d776191f40db88d7a0e0b4544279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 38416.24, Learning Rate = 2.00e-01\n",
      "Epoch: 1, Loss = 84251.29, Learning Rate = 4.00e-01\n",
      "Epoch: 2, Loss = 121610.12, Learning Rate = 6.00e-01\n",
      "Epoch: 3, Loss = 165575.69, Learning Rate = 8.00e-01\n",
      "Epoch: 4, Loss = 229921.73, Learning Rate = 1.00e+00\n",
      "\n",
      "2e-06\t6244.56\n",
      "4e-06\t5926.94\n",
      "6e-06\t5830.10\n",
      "8e-06\t5766.99\n",
      "1e-05\t5761.07\n",
      "2e-05\t5547.02\n",
      "4e-05\t5400.37\n",
      "6e-05\t5379.64\n",
      "8e-05\t5304.28\n",
      "1e-04\t5252.24\n",
      "2e-04\t5216.68\n",
      "4e-04\t5138.04\n",
      "6e-04\t5089.22\n",
      "8e-04\t5098.34\n",
      "1e-03\t4983.20\n",
      "2e-03\t5140.37\n",
      "4e-03\t5198.16\n",
      "6e-03\t5744.86\n",
      "8e-03\t5603.75\n",
      "1e-02\t5859.97\n",
      "2e-02\t7149.28\n",
      "4e-02\t9230.91\n",
      "6e-02\t11248.39\n",
      "8e-02\t16239.48\n",
      "1e-01\t21634.04\n",
      "2e-01\t38416.24\n",
      "4e-01\t84251.29\n",
      "6e-01\t121610.12\n",
      "8e-01\t165575.69\n",
      "1e+00\t229921.73\n"
     ]
    }
   ],
   "source": [
    "# Training with K-fold\n",
    "new_data = data\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 5\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Freeze BERT\n",
    "model.freeze()\n",
    "\n",
    "\"\"\" TRAINING \"\"\"\n",
    "dataset = BonzDataset(data.iloc[:,-3:], None)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "''' STORING WHILE TRAINING'''\n",
    "lr_list = []\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(6):\n",
    "    # Setup scheduler each period\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  base_lr=0, \n",
    "                                                  max_lr=LEARNING_RATE*10, \n",
    "                                                  step_size_up=EPOCH, \n",
    "                                                  cycle_momentum=False)\n",
    "    scheduler.step()\n",
    "    \n",
    "    for epoch in tqdm.notebook.trange(EPOCH):\n",
    "\n",
    "        # Load original weights\n",
    "        model.load_state_dict(origin_sd) \n",
    "        loss_train = 0\n",
    "\n",
    "        for idx, (a, b, c) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            predict, loss = model(a.to(DEVICE), \n",
    "                                  b.to(DEVICE), \n",
    "                                  c.to(DEVICE), \n",
    "                                  weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "            loss.backward()\n",
    "            loss_train += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "\n",
    "        # Store metrics\n",
    "        lr_list.append(current_lr)\n",
    "        loss_list.append(loss_train)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    LEARNING_RATE *= 10\n",
    "\n",
    "    \n",
    "# Print losss per learning rate\n",
    "for a, b in zip(lr_list, loss_list):\n",
    "    print(f'{a:.0e}\\t{b:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3c1f4534f14389b14e123571bda099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a488963d330b46418acae0e2ddaa30e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4423.82\n",
      "Epoch: 1, Loss = 3950.26\n",
      "Epoch: 2, Loss = 3725.88\n",
      "Epoch: 3, Loss = 3474.10\n",
      "Epoch: 4, Loss = 3177.60\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0964c907f694b339c275248c94e9f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4504.44\n",
      "Epoch: 1, Loss = 4183.29\n",
      "Epoch: 2, Loss = 3992.96\n",
      "Epoch: 3, Loss = 3821.90\n",
      "Epoch: 4, Loss = 3661.60\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6821fcc851594c87837cfaf03f9c525a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4355.87\n",
      "Epoch: 1, Loss = 3910.19\n",
      "Epoch: 2, Loss = 3676.94\n",
      "Epoch: 3, Loss = 3424.62\n",
      "Epoch: 4, Loss = 3136.30\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f79f338ba4054b54ee5f8952340a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4321.13\n",
      "Epoch: 1, Loss = 3876.30\n",
      "Epoch: 2, Loss = 3656.44\n",
      "Epoch: 3, Loss = 3391.58\n",
      "Epoch: 4, Loss = 3085.10\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6f378e0f78460f803ad0dc4eb3cd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4403.65\n",
      "Epoch: 1, Loss = 3983.06\n",
      "Epoch: 2, Loss = 3770.46\n",
      "Epoch: 3, Loss = 3549.74\n",
      "Epoch: 4, Loss = 3304.46\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5c9e178ab44852895cfa99f5fe93c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4430.94\n",
      "Epoch: 1, Loss = 3881.30\n",
      "Epoch: 2, Loss = 3646.53\n",
      "Epoch: 3, Loss = 3418.56\n",
      "Epoch: 4, Loss = 3144.37\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2ea98060e34a7ebe15d975905036fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4310.70\n",
      "Epoch: 1, Loss = 3850.37\n",
      "Epoch: 2, Loss = 3602.55\n",
      "Epoch: 3, Loss = 3324.71\n",
      "Epoch: 4, Loss = 3024.26\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b98c2d6e3c8429ea5083bc228505aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4700.06\n",
      "Epoch: 1, Loss = 4238.62\n",
      "Epoch: 2, Loss = 4029.17\n",
      "Epoch: 3, Loss = 3881.45\n",
      "Epoch: 4, Loss = 3719.69\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3ffd59313e4f95949541d41e50e440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4347.60\n",
      "Epoch: 1, Loss = 3920.74\n",
      "Epoch: 2, Loss = 3693.45\n",
      "Epoch: 3, Loss = 3463.86\n",
      "Epoch: 4, Loss = 3192.02\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13894f7dec2a403a81b5794d250f20c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 4513.03\n",
      "Epoch: 1, Loss = 3919.64\n",
      "Epoch: 2, Loss = 3674.30\n",
      "Epoch: 3, Loss = 3408.24\n",
      "Epoch: 4, Loss = 3103.42\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training with K-fold\n",
    "#new_data = data.sample(frac=1).reset_index(drop=True)\n",
    "new_data = data\n",
    "kf = KFold(10)\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "NUM_TRAINING_STEPS = 73 * 5\n",
    "NUM_TRAINING_STEPS//10\n",
    "\n",
    "last_predict = []\n",
    "i = 0\n",
    "for train_idx, test_idx in tqdm.notebook.tqdm(kf.split(new_data)):\n",
    "    train_data = new_data.iloc[train_idx]\n",
    "    test_data = new_data.iloc[test_idx]\n",
    "    \n",
    "    print(model.load_state_dict(origin_sd))\n",
    "    \n",
    "    ''' Get optimizer for each KFold\n",
    "    optimizer = optimizer_list[i]\n",
    "    optimizer_list[i] = ''\n",
    "    i += 1\n",
    "    '''\n",
    "    '''\n",
    "    NUM_TRAINING_STEPS = (new_data.shape[0]//BATCH_SIZE + 1) * EPOCH\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, NUM_TRAINING_STEPS//10, NUM_TRAINING_STEPS)\n",
    "    '''\n",
    "\n",
    "    \"\"\" TRAINING \"\"\"\n",
    "    dataset = BonzDataset(train_data.iloc[:,-3:], None)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    model.train()\n",
    "    for epoch in tqdm.notebook.trange(EPOCH):\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        loss_train = 0\n",
    "        for idx, (a, b, c) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            predict, loss = model(a.to(DEVICE), \n",
    "                                  b.to(DEVICE), \n",
    "                                  c.to(DEVICE), \n",
    "                                  weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "            #predict, loss = model(a.to(DEVICE), None, c.to(DEVICE))[:2]   # This is normal BERT\n",
    "\n",
    "            ''' Using apex fp16 loss \n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            '''\n",
    "\n",
    "            ''' normal loss '''\n",
    "            loss.backward()\n",
    "\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss = {loss_train:.2f}')\n",
    "                \n",
    "        \n",
    "    ''' TESTING  ''' \n",
    "    model.eval()\n",
    "    dataset = BonzDataset(test_data.iloc[:,-3:], None)\n",
    "    dataloader = DataLoader(dataset, batch_size=40)\n",
    "\n",
    "    for idx, (a, b, c) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            predict = model(a.to(DEVICE), b.to(DEVICE))[0] # This is L-BERT\n",
    "            #predict = model(a.to(DEVICE), None)[0] # This is normal BERT\n",
    "        last_predict.extend(predict.detach().cpu().numpy().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080e21fa79034b2fb5b5e8e8d97db501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#sd = torch.load('./saved_state_dict/epoch5lr2e5.pth')\n",
    "#model.load_state_dict(sd)\n",
    "model.eval()\n",
    "\n",
    "dataset = BonzDataset(new_data.iloc[:,-3:], None)\n",
    "dataloader = DataLoader(dataset, batch_size=40)\n",
    "\n",
    "last_predict = []\n",
    "for idx, (a, b, c) in enumerate(tqdm.notebook.tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        predict = model(a.to(DEVICE), b.to(DEVICE))[0]\n",
    "    last_predict.extend(predict.detach().cpu().numpy().tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.46      0.51     10949\n",
      "           1       0.82      0.88      0.85     31161\n",
      "           2       0.27      0.21      0.23       757\n",
      "\n",
      "    accuracy                           0.76     42867\n",
      "   macro avg       0.56      0.52      0.53     42867\n",
      "weighted avg       0.75      0.76      0.75     42867\n",
      "\n",
      "SIT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58     11434\n",
      "           1       0.84      0.89      0.86     30804\n",
      "           2       0.26      0.22      0.24       629\n",
      "\n",
      "    accuracy                           0.79     42867\n",
      "   macro avg       0.58      0.55      0.56     42867\n",
      "weighted avg       0.78      0.79      0.78     42867\n",
      "\n",
      "ENT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.49      0.52     13359\n",
      "           1       0.78      0.81      0.79     24475\n",
      "           2       0.36      0.40      0.38      5033\n",
      "\n",
      "    accuracy                           0.66     42867\n",
      "   macro avg       0.56      0.57      0.56     42867\n",
      "weighted avg       0.66      0.66      0.66     42867\n",
      "\n",
      "CUS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      7551\n",
      "           1       0.93      0.94      0.94     34649\n",
      "           2       0.06      0.02      0.03       667\n",
      "\n",
      "    accuracy                           0.89     42867\n",
      "   macro avg       0.57      0.56      0.56     42867\n",
      "weighted avg       0.88      0.89      0.88     42867\n",
      "\n",
      "VOM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65      9302\n",
      "           1       0.88      0.92      0.90     32264\n",
      "           2       0.16      0.10      0.12      1301\n",
      "\n",
      "    accuracy                           0.83     42867\n",
      "   macro avg       0.58      0.54      0.56     42867\n",
      "weighted avg       0.82      0.83      0.82     42867\n",
      "\n",
      "CLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.50      0.47      4734\n",
      "           1       0.82      0.84      0.83     30667\n",
      "           2       0.43      0.36      0.39      7466\n",
      "\n",
      "    accuracy                           0.71     42867\n",
      "   macro avg       0.56      0.56      0.56     42867\n",
      "weighted avg       0.71      0.71      0.71     42867\n",
      "\n",
      "CKI:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50      5348\n",
      "           1       0.81      0.84      0.82     30135\n",
      "           2       0.42      0.35      0.38      7384\n",
      "\n",
      "    accuracy                           0.71     42867\n",
      "   macro avg       0.57      0.57      0.57     42867\n",
      "weighted avg       0.70      0.71      0.71     42867\n",
      "\n",
      "FNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56     11553\n",
      "           1       0.70      0.74      0.72     22887\n",
      "           2       0.42      0.36      0.39      8427\n",
      "\n",
      "    accuracy                           0.62     42867\n",
      "   macro avg       0.56      0.55      0.56     42867\n",
      "weighted avg       0.61      0.62      0.61     42867\n",
      "\n",
      "LEG, 55.53,    51.76,    53.27,    76.34\n",
      "SIT, 58.11,    54.86,    56.21,    78.75\n",
      "ENT, 56.39,    56.67,    56.41,    66.08\n",
      "CUS, 57.33,    56.01,    56.37,    88.90\n",
      "VOM, 58.15,    54.46,    55.90,    83.14\n",
      "CLE, 56.21,    56.38,    56.13,    71.48\n",
      "CKI, 57.31,    56.57,    56.84,    71.28\n",
      "FNB, 56.14,    55.20,    55.58,    61.70\n"
     ]
    }
   ],
   "source": [
    "last_predict_ = torch.tensor(last_predict)\n",
    "last_predict_ = torch.softmax(last_predict_, 1)\n",
    "y_predict = torch.argmax(last_predict_, 1)\n",
    "y_true = np.asarray(list(new_data.labels))\n",
    "\n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}:\\n{classification_report(y_true[:,i], y_predict[:,i])}')\n",
    "    \n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}, {precision_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {accuracy_score(y_true[:,i], y_predict[:,i])*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(y_predict, 'result/LCAT_5epoch_1e5_WeightClass_xavier.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
