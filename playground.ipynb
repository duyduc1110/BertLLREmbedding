{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/udacity-pytorch-challengers/ideas-on-how-to-fine-tune-a-pre-trained-model-in-pytorch-184c47185a20\n",
    "# https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertTokenizerFast, BertModel, AdamW, TFBertModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import BertEmbeddings, BertSelfAttention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from apex import amp, optimizers\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "COL_NAMES = ['TopNumber', 'AirlineName','ReviewerName','Rating','ReviewDate','ReviewTitle',\\\n",
    "             'ReviewText','Tags', 'DateofTravel', 'Aspects', 'ResponserName', 'ResponseDate', 'ResponseText', 'ReviewerProfileUrl',\\\n",
    "             'AirlineNation', 'CrawlTime']\n",
    "\n",
    "PRE_TRAINED = 'bert-base-uncased'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASPECT_NAMES = ['LEG', 'SIT', 'ENT', 'CUS', 'VOM', 'CLE', 'CKI', 'FNB']\n",
    "VOCAB_DIC = BertTokenizerFast.from_pretrained(PRE_TRAINED).get_vocab()\n",
    "TOPN = 150\n",
    "\n",
    "\n",
    "# This one is implemented with weight loss per class            \n",
    "class BertBonzWeightLoss(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertBonzWeightLoss, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings.llr_embeddings = nn.ModuleList(nn.Embedding(4, 768, 3) for _ in range(len(ASPECT_NAMES)))\n",
    "        self.classifier = nn.Linear(768, config.num_aspect*3)\n",
    "        self.init_weights()\n",
    "        self.embeddings.llr_embeddings.apply(self._xavier)\n",
    "        self.pooler.apply(self._xavier)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                llr_ids=None, \n",
    "                labels=None, \n",
    "                token_type_ids=None, \n",
    "                position_ids=None,\n",
    "                weight_loss=None):\n",
    "        # BERT EMBEDDINGS NEW\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs_embeds = self.embeddings.word_embeddings(input_ids)\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        if llr_ids is not None:\n",
    "            temp = [self.embeddings.llr_embeddings[i](llr_ids[:,i,:]) for i in range(self.config.num_aspect)]\n",
    "            llr_embeddings = sum(temp)\n",
    "        else:\n",
    "            llr_embeddings = torch.zeros(inputs_embeds.size(), device=device).fill_(3).long()\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + llr_embeddings\n",
    "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.embeddings.dropout(embeddings)\n",
    "        \n",
    "        \n",
    "        # BERT ENCODER\n",
    "        encoder_outputs = self.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=None,\n",
    "            head_mask=[None]*12,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=self.config.output_attentions\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # CLASSIFIER\n",
    "        CLS_token = sequence_output[:,0]\n",
    "        predict = self.classifier(CLS_token)\n",
    "        \n",
    "        loss_fn = nn.functional.cross_entropy\n",
    "        if labels is not None:\n",
    "            if weight_loss is None:\n",
    "                loss = loss_fn(predict.view(input_shape[0], 3,-1), labels)\n",
    "            else:\n",
    "                loss = torch.tensor(0).float().to(DEVICE)\n",
    "                for asp_i in range(len(ASPECT_NAMES)):\n",
    "                    loss += loss_fn(predict.view(input_shape[0], 3,-1)[:,:,asp_i], labels[:,asp_i], weight_loss[asp_i, :])\n",
    "                loss /= len(ASPECT_NAMES)\n",
    "                    \n",
    "            outputs = (predict.view(input_shape[0], 3,-1), loss, CLS_token, sequence_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        else:\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), CLS_token, sequence_output) + encoder_outputs[1:]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def load_pretrained_weight(self):\n",
    "        sd = self.state_dict()\n",
    "        sd_bert_pretrained = BertModel.from_pretrained(PRE_TRAINED).state_dict()\n",
    "        for k in sd_bert_pretrained.keys():\n",
    "            if k in sd.keys():\n",
    "                sd[k] = sd_bert_pretrained[k]\n",
    "        self.load_state_dict(sd)\n",
    "        print('Succesfully load pre-trained weights')\n",
    "        \n",
    "    def llr_embed_pad(self):\n",
    "        for i in range(len(self.embeddings.llr_embeddings)):\n",
    "            temp = self.embeddings.llr_embeddings[i].weight.data\n",
    "            temp[-1,:] = torch.zeros(temp.size(1))\n",
    "        \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    "    def unfreeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "                \n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.embeddings.llr_embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True            \n",
    "    \n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data, llr_words):\n",
    "        self.input_ids = torch.LongTensor(list(data.input_ids))\n",
    "        self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        if 'llr_embeddings' in data.columns:\n",
    "            self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        else:\n",
    "            self.llr_embeddings = torch.zeros(data.shape[0],1).long()\n",
    "        if 'labels' in data.columns:\n",
    "            self.labels = torch.LongTensor(list(data.labels))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.llr_words = llr_words\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        tokens = self.data.input_ids[idx]\n",
    "        \n",
    "        llr_embedding = []\n",
    "        for aspect in ASPECT_NAMES:\n",
    "            temp = [3] * tokens.shape[0]\n",
    "            for j in range(tokens.shape[0]):\n",
    "                for class_, wordlist in llr_words[aspect].items():\n",
    "                    if tokens[j] in wordlist:\n",
    "                        temp[j] = class_\n",
    "                        break\n",
    "            llr_embedding.append(temp)\n",
    "        \n",
    "        llr_embedding = torch.stack([torch.LongTensor(i) for i in llr_embedding], 0)\n",
    "        \n",
    "        \n",
    "        outputs = (torch.LongTensor(tokens), llr_embedding)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            outputs = (torch.LongTensor(tokens), llr_embedding, torch.LongTensor(self.data.labels[idx]))\n",
    "        '''\n",
    "        if self.labels is None:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx])\n",
    "        else:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx], self.labels[idx])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    \n",
    "def split_aspect(data):\n",
    "    temp = np.full((8, data.shape[0]), 2, np.int)\n",
    "    for idx in range(data.shape[0]):\n",
    "        aspect = data[idx]\n",
    "        for i, asp in enumerate(['Legroom', 'Seat', 'Entertainment', 'Customer', 'Value', 'Cleanliness', 'Check-in', 'Food']):\n",
    "            for sub_asp in aspect:\n",
    "                if asp in sub_asp:\n",
    "                    pol = int(sub_asp[-1])\n",
    "                    temp[i, idx] = 1 if pol > 3 else 0\n",
    "                    break\n",
    "    return temp\n",
    "            \n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED)\n",
    "    input_ids = tokenizer(list(data))['input_ids']\n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, padding='post', truncating='post')\n",
    "    \n",
    "    return (list(input_ids), tokenizer)\n",
    "    \n",
    "    \n",
    "def get_data(FILE_PATH, COL_NAMES):\n",
    "    raw_data = pd.read_csv(FILE_PATH, sep='\\t', header=None, names=COL_NAMES)\n",
    "    data = raw_data[['ReviewText', 'Rating', 'Aspects']]\n",
    "    data = data[data['Aspects'] != 'No filling in'] # Filter none aspects\n",
    "    data.Aspects = data.Aspects.str.split('|').values\n",
    "    \n",
    "    '''Split aspects to new columns'''\n",
    "    aspects_splitted = split_aspect(data.Aspects.values)\n",
    "    for i in range(len(ASPECT_NAMES)):\n",
    "        data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "        \n",
    "    data['input_ids'], tokenizer = tokenize_data(data.ReviewText.values) # Generate input_ids from review text\n",
    "    \n",
    "    return data, tokenizer\n",
    "\n",
    "\n",
    "def word_class_freq(data, aspect_name, aspect_class=3):\n",
    "    temp = np.zeros((33000, aspect_class), np.int)\n",
    "    ids = data.input_ids.values\n",
    "    labels = data[aspect_name].values\n",
    "\n",
    "    for sub_ids, sub_lb in zip(ids, labels):\n",
    "        set_ids = set(sub_ids)\n",
    "        for ids in set_ids:\n",
    "            temp[ids, sub_lb] += 1\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def calculate_llr(temp_df, labels):\n",
    "    N = data.shape[0]\n",
    "    total_scores = []\n",
    "\n",
    "    for i in temp_df.index.values:\n",
    "        llr_scores = []\n",
    "        for class_ in [0,1,2]:\n",
    "            num_class_doc = np.sum(labels == class_)\n",
    "            n11 = temp_df.loc[i, class_]\n",
    "            n10 = num_class_doc - n11\n",
    "            n01 = temp_df.loc[i, 'total'] - n11\n",
    "            n00 = (N - n11 - n10 - n01)\n",
    "            pt = (1e-10 + n11 + n01)/N\n",
    "            p1 = n11/(1e-10 + n11 + n10)\n",
    "            p2 = n01/(1e-10 + n01 + n00)\n",
    "\n",
    "\n",
    "            try:\n",
    "                e1 = n11 * (math.log(pt) - math.log(p1))\n",
    "            except:\n",
    "                e1 = 0\n",
    "            try:\n",
    "                e2 = n10 * (math.log(1-pt) - math.log(1-p1))\n",
    "            except:\n",
    "                e2 = 0\n",
    "            try:\n",
    "                e3 = n01 * (math.log(pt) - math.log(p2))\n",
    "            except:\n",
    "                e3 = 0\n",
    "            try:\n",
    "                e4 = n00 * (math.log(1-pt) - math.log(1-p2))\n",
    "            except:\n",
    "                e4 = 0\n",
    "\n",
    "            llr_score = -2 * (e1+e2+e3+e4)\n",
    "            if n11 < n01:\n",
    "                llr_score = 0\n",
    "            llr_scores.append(llr_score)\n",
    "\n",
    "        total_scores.append(llr_scores)\n",
    "    \n",
    "    llr_df = pd.DataFrame(np.array(total_scores), index=temp_df.index, columns=temp_df.columns.values[:-1])\n",
    "\n",
    "    return llr_df\n",
    "\n",
    "\n",
    "def generate_llr_score(data, aspect):\n",
    "    temp = word_class_freq(data, aspect)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp)\n",
    "    temp_df['total'] = np.sum(temp, -1)\n",
    "    temp_df = temp_df[temp_df['total'] != 0]\n",
    "    temp_df = temp_df.drop(0,0)\n",
    "    \n",
    "    return calculate_llr(temp_df, data[aspect].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PRE-PROCESSED DATA (IF ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceefa27dce684979b2274de3e2e99e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca96b5c57854be889a497eb6f5f5ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=152574.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d448df60df7148e1a328aa89e9114810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=152574.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aead9a8b07b498dad2f3541a9e847ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=152574.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1314, 0.1293, 0.7393],\n",
       "        [0.1193, 0.1175, 0.7633],\n",
       "        [0.3078, 0.3041, 0.3881],\n",
       "        [0.1410, 0.1291, 0.7299],\n",
       "        [0.2203, 0.2106, 0.5691],\n",
       "        [0.3805, 0.3023, 0.3173],\n",
       "        [0.3706, 0.3069, 0.3225],\n",
       "        [0.3339, 0.3270, 0.3391]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load n process read data\n",
    "data = pd.read_csv('./data/pre-processed_150_v3.csv', sep='\\t', index_col=0)\n",
    "\n",
    "for col in tqdm.notebook.tqdm(['input_ids', 'labels', 'llr_embeddings']):\n",
    "    data[col] = [ast.literal_eval(i) for i in tqdm.notebook.tqdm(data[col].values)]\n",
    "\n",
    "\n",
    "# CALCULATE WEIGHT LOSS\n",
    "labels = pd.DataFrame([i for i in data.labels])\n",
    "beta = 0.9999\n",
    "#beta = (data.shape[0] - 1) / data.shape[0]\n",
    "weight_loss = []\n",
    "\n",
    "for i in labels:\n",
    "    n_sample = labels.loc[:, i].value_counts(0, 0).values\n",
    "    n_sample = 1.0 - np.power(beta, n_sample)\n",
    "    n_sample = (1.0 - beta) / n_sample\n",
    "    n_sample = n_sample / np.sum(n_sample)\n",
    "    weight_loss.append(n_sample)\n",
    "\n",
    "weight_loss = torch.tensor(weight_loss, device=DEVICE).float()\n",
    "weight_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'index': data.index}\n",
    "for col in data.columns:\n",
    "    new_data[col] = torch.tensor(list(data[col]))\n",
    "\n",
    "\n",
    "torch.save(new_data, './data/pre-processed_150_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>llr_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152574 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_ids  \\\n",
       "0       [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...   \n",
       "1       [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...   \n",
       "2       [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...   \n",
       "3       [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...   \n",
       "4       [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...   \n",
       "...                                                   ...   \n",
       "190981  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...   \n",
       "190982  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...   \n",
       "190983  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...   \n",
       "190984  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...   \n",
       "190985  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...   \n",
       "\n",
       "                          labels  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4       [1, 1, 1, 0, 1, 1, 1, 1]   \n",
       "...                          ...   \n",
       "190981  [0, 0, 0, 0, 0, 2, 2, 2]   \n",
       "190982  [1, 1, 1, 1, 1, 2, 2, 2]   \n",
       "190983  [1, 1, 1, 1, 1, 2, 2, 2]   \n",
       "190984  [0, 1, 1, 0, 0, 2, 2, 2]   \n",
       "190985  [1, 1, 0, 0, 0, 2, 2, 2]   \n",
       "\n",
       "                                           llr_embeddings  \n",
       "0       [[3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1,...  \n",
       "1       [[3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "2       [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "3       [[3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "4       [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "...                                                   ...  \n",
       "190981  [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,...  \n",
       "190982  [[3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3,...  \n",
       "190983  [[3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 1,...  \n",
       "190984  [[3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "190985  [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "\n",
       "[152574 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.load('./data/pre-processed_150_v3.pt')\n",
    "for col in a:\n",
    "    a[col] = a[col].tolist()\n",
    "\n",
    "data = pd.DataFrame(a, a['index']).drop('index', 1)\n",
    "\n",
    "# CALCULATE WEIGHT LOSS\n",
    "labels = pd.DataFrame([i for i in data.labels])\n",
    "beta = 0.9999\n",
    "#beta = (data.shape[0] - 1) / data.shape[0]\n",
    "weight_loss = []\n",
    "\n",
    "for i in labels:\n",
    "    n_sample = labels.loc[:, i].value_counts(0, 0).values\n",
    "    n_sample = 1.0 - np.power(beta, n_sample)\n",
    "    n_sample = (1.0 - beta) / n_sample\n",
    "    n_sample = n_sample / np.sum(n_sample)\n",
    "    weight_loss.append(n_sample)\n",
    "\n",
    "weight_loss = torch.tensor(weight_loss, device=DEVICE).float()\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190986, 16)\n",
      "(152574, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With everyone trying to get home in the Covid ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ad a lot of people did, we had to scramble to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After coming into Changi airport and worrying ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>We booked to fly from Heathrow to Newark. The ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:2, Seat comfort:2, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>Love Virgin, great staff, food good, quality o...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>Virgin upper class is outstanding, really very...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>Virgins premium economy is the best I have com...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:3, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>At my age I cannot face an overnight in econom...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152574 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ReviewText  Rating  \\\n",
       "0       With everyone trying to get home in the Covid ...       4   \n",
       "1       Ad a lot of people did, we had to scramble to ...       5   \n",
       "2       After coming into Changi airport and worrying ...       4   \n",
       "3       Great service, great plane, great pricing. We ...       4   \n",
       "4       My husband and I were to fly home from Houston...       1   \n",
       "...                                                   ...     ...   \n",
       "190981  We booked to fly from Heathrow to Newark. The ...       1   \n",
       "190982  Love Virgin, great staff, food good, quality o...       5   \n",
       "190983  Virgin upper class is outstanding, really very...       5   \n",
       "190984  Virgins premium economy is the best I have com...       5   \n",
       "190985  At my age I cannot face an overnight in econom...       2   \n",
       "\n",
       "                                                  Aspects  LEG  SIT  ENT  CUS  \\\n",
       "0       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "1       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "2       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "3       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "4       [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    0   \n",
       "...                                                   ...  ...  ...  ...  ...   \n",
       "190981  [Legroom:2, Seat comfort:2, In-flight Entertai...    0    0    0    0   \n",
       "190982  [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "190983  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "190984  [Legroom:3, Seat comfort:5, In-flight Entertai...    0    1    1    0   \n",
       "190985  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    0    0   \n",
       "\n",
       "        VOM  CLE  CKI  FNB                                          input_ids  \\\n",
       "0         1    1    1    1  [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...   \n",
       "1         1    1    1    1  [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...   \n",
       "2         1    1    1    1  [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...   \n",
       "3         1    1    1    1  [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...   \n",
       "4         1    1    1    1  [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...   \n",
       "...     ...  ...  ...  ...                                                ...   \n",
       "190981    0    2    2    2  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...   \n",
       "190982    1    2    2    2  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...   \n",
       "190983    1    2    2    2  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...   \n",
       "190984    0    2    2    2  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...   \n",
       "190985    0    2    2    2  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...   \n",
       "\n",
       "                          labels  \n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4       [1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "...                          ...  \n",
       "190981  [0, 0, 0, 0, 0, 2, 2, 2]  \n",
       "190982  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190983  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190984  [0, 1, 1, 0, 0, 2, 2, 2]  \n",
       "190985  [1, 1, 0, 0, 0, 2, 2, 2]  \n",
       "\n",
       "[152574 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, tokenizer = get_data('./data/data_v3.txt', COL_NAMES)\n",
    "data['labels'] = list(data.iloc[:, 3:11].values)\n",
    "\n",
    "# CALCULATE WEIGHT LOSS\n",
    "labels = pd.DataFrame([i for i in data.labels])\n",
    "beta = 0.9999\n",
    "#beta = (data.shape[0] - 1) / data.shape[0]\n",
    "weight_loss = []\n",
    "\n",
    "for i in labels:\n",
    "    n_sample = labels.loc[:, i].value_counts(0, 0).values\n",
    "    n_sample = 1.0 - np.power(beta, n_sample)\n",
    "    n_sample = (1.0 - beta) / n_sample\n",
    "    n_sample = n_sample / np.sum(n_sample)\n",
    "    weight_loss.append(n_sample)\n",
    "\n",
    "weight_loss = torch.tensor(weight_loss, device=DEVICE).float()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE LLR SCORES & WORDLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b674bec727347729a3ed2c68b4ebe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Calculate LLR scores', max=8.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af446498d1d344e7b966241b85cdf26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate top LLR words', max=8.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f5d1c956674d5cb147217c66bd85a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=152574.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>llr_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With everyone trying to get home in the Covid ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ad a lot of people did, we had to scramble to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After coming into Changi airport and worrying ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>We booked to fly from Heathrow to Newark. The ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:2, Seat comfort:2, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>Love Virgin, great staff, food good, quality o...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>Virgin upper class is outstanding, really very...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>Virgins premium economy is the best I have com...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:3, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>At my age I cannot face an overnight in econom...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 2]</td>\n",
       "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152574 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ReviewText  Rating  \\\n",
       "0       With everyone trying to get home in the Covid ...       4   \n",
       "1       Ad a lot of people did, we had to scramble to ...       5   \n",
       "2       After coming into Changi airport and worrying ...       4   \n",
       "3       Great service, great plane, great pricing. We ...       4   \n",
       "4       My husband and I were to fly home from Houston...       1   \n",
       "...                                                   ...     ...   \n",
       "190981  We booked to fly from Heathrow to Newark. The ...       1   \n",
       "190982  Love Virgin, great staff, food good, quality o...       5   \n",
       "190983  Virgin upper class is outstanding, really very...       5   \n",
       "190984  Virgins premium economy is the best I have com...       5   \n",
       "190985  At my age I cannot face an overnight in econom...       2   \n",
       "\n",
       "                                                  Aspects  LEG  SIT  ENT  CUS  \\\n",
       "0       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "1       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "2       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "3       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "4       [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    0   \n",
       "...                                                   ...  ...  ...  ...  ...   \n",
       "190981  [Legroom:2, Seat comfort:2, In-flight Entertai...    0    0    0    0   \n",
       "190982  [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "190983  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "190984  [Legroom:3, Seat comfort:5, In-flight Entertai...    0    1    1    0   \n",
       "190985  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    0    0   \n",
       "\n",
       "        VOM  CLE  CKI  FNB                                          input_ids  \\\n",
       "0         1    1    1    1  [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...   \n",
       "1         1    1    1    1  [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...   \n",
       "2         1    1    1    1  [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...   \n",
       "3         1    1    1    1  [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...   \n",
       "4         1    1    1    1  [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...   \n",
       "...     ...  ...  ...  ...                                                ...   \n",
       "190981    0    2    2    2  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...   \n",
       "190982    1    2    2    2  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...   \n",
       "190983    1    2    2    2  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...   \n",
       "190984    0    2    2    2  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...   \n",
       "190985    0    2    2    2  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...   \n",
       "\n",
       "                          labels  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4       [1, 1, 1, 0, 1, 1, 1, 1]   \n",
       "...                          ...   \n",
       "190981  [0, 0, 0, 0, 0, 2, 2, 2]   \n",
       "190982  [1, 1, 1, 1, 1, 2, 2, 2]   \n",
       "190983  [1, 1, 1, 1, 1, 2, 2, 2]   \n",
       "190984  [0, 1, 1, 0, 0, 2, 2, 2]   \n",
       "190985  [1, 1, 0, 0, 0, 2, 2, 2]   \n",
       "\n",
       "                                           llr_embeddings  \n",
       "0       [[3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1,...  \n",
       "1       [[3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "2       [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "3       [[3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "4       [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "...                                                   ...  \n",
       "190981  [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,...  \n",
       "190982  [[3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3,...  \n",
       "190983  [[3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 1,...  \n",
       "190984  [[3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,...  \n",
       "190985  [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,...  \n",
       "\n",
       "[152574 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords in English\n",
    "stopwords_ids = tokenizer.convert_tokens_to_ids(stopwords.words('english'))\n",
    "\n",
    "llr_scores = {}\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Calculate LLR scores'):\n",
    "    llr_df = generate_llr_score(data, aspect)\n",
    "    \n",
    "    # Clear stopword ids\n",
    "    llr_df = llr_df.drop(stopwords_ids, 0)\n",
    "    \n",
    "    llr_scores[aspect] = llr_df\n",
    "\n",
    "\n",
    "llr_words = dict()\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Generate top LLR words'):\n",
    "    kw_label = dict()\n",
    "    for class_ in [0,1,2]:\n",
    "        # Sort keywords based on aspect, class and top_n words\n",
    "        kw_list = list(llr_scores[aspect][class_].sort_values(ascending=False)[:TOPN].index)\n",
    "        \n",
    "        kw_label[class_] = kw_list\n",
    "        \n",
    "    llr_words[aspect] = kw_label\n",
    "\n",
    "llr_embedding_list = []\n",
    "\n",
    "for idx in tqdm.notebook.tqdm(data.index):\n",
    "    tokens = data.input_ids[idx]\n",
    "    \n",
    "    llr_embedding = []\n",
    "    for aspect in ASPECT_NAMES:\n",
    "        temp = [3] * tokens.shape[0]\n",
    "        for j in range(tokens.shape[0]):\n",
    "            for class_, wordlist in llr_words[aspect].items():\n",
    "                if tokens[j] in wordlist:\n",
    "                    temp[j] = class_\n",
    "                    break\n",
    "        llr_embedding.append(temp)\n",
    "    \n",
    "    llr_embedding_list.append(llr_embedding)\n",
    "\n",
    "#data['llr_embeddings'] = [[[0]*512]*8] * data.shape[0]\n",
    "data['llr_embeddings'] = llr_embedding_list\n",
    "\n",
    "# Turn numpy array to list to store easier\n",
    "for i in data.keys()[-3:]:\n",
    "    data[i] = data[i].map(list)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,-3:].to_csv('./data/pre-processed_150_v3.csv', sep='\\t')\n",
    "data.iloc[:1000,-3:].to_csv('./data/sample_150_v3.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_aspect = len(ASPECT_NAMES)\n",
    "model = BertBonzWeightLoss(config)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.load_pretrained_weight() # Load pre-trained BERT weights for BERT's layers \n",
    "model.llr_embed_pad() # Set LLR embedding padding idx to 0-value tensor\n",
    "\n",
    "\n",
    "\n",
    "''' Using apex for faster training\n",
    "optimizer_list = []\n",
    "for i in range(10):\n",
    "    optimizer_list.append(AdamW(model.parameters(), lr=3e-5, correct_bias=False))\n",
    "\n",
    "model = amp.initialize(model, opt_level=\"O2\", verbosity=0)\n",
    "''' \n",
    "\n",
    "''' Save origin state dict of Model and Optimizer'''\n",
    "#torch.save(model.state_dict(), 'origin_sd.pth')\n",
    "origin_sd = torch.load('./state_dict/freeze.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Learning-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07e-07,0.8326\n",
      "1.15e-07,0.8241\n",
      "1.23e-07,0.8335\n",
      "1.32e-07,0.8196\n",
      "1.41e-07,0.8121\n",
      "1.51e-07,0.7947\n",
      "1.62e-07,0.7743\n",
      "1.74e-07,0.7625\n",
      "1.86e-07,0.7493\n",
      "2.00e-07,0.7390\n",
      "2.14e-07,0.7329\n",
      "2.29e-07,0.7304\n",
      "2.45e-07,0.7064\n",
      "2.63e-07,0.6962\n",
      "2.82e-07,0.6835\n",
      "3.02e-07,0.6786\n",
      "3.24e-07,0.6720\n",
      "3.47e-07,0.6671\n",
      "3.72e-07,0.6766\n",
      "3.98e-07,0.6653\n",
      "4.27e-07,0.6769\n",
      "4.57e-07,0.6708\n",
      "4.90e-07,0.6823\n",
      "5.25e-07,0.6745\n",
      "5.62e-07,0.6962\n",
      "6.03e-07,0.6905\n",
      "6.46e-07,0.6836\n",
      "6.92e-07,0.6770\n",
      "7.41e-07,0.6663\n",
      "7.94e-07,0.6585\n",
      "8.51e-07,0.6620\n",
      "9.12e-07,0.6713\n",
      "9.77e-07,0.6643\n",
      "1.05e-06,0.6492\n",
      "1.12e-06,0.6377\n",
      "1.20e-06,0.6426\n",
      "1.29e-06,0.6377\n",
      "1.38e-06,0.6262\n",
      "1.48e-06,0.6246\n",
      "1.58e-06,0.6175\n",
      "1.70e-06,0.6237\n",
      "1.82e-06,0.6112\n",
      "1.95e-06,0.6040\n",
      "2.09e-06,0.5995\n",
      "2.24e-06,0.5996\n",
      "2.40e-06,0.6003\n",
      "2.57e-06,0.6070\n",
      "2.75e-06,0.6068\n",
      "2.95e-06,0.6097\n",
      "3.16e-06,0.6106\n",
      "3.39e-06,0.6096\n",
      "3.63e-06,0.6079\n",
      "3.89e-06,0.6146\n",
      "4.17e-06,0.6151\n",
      "4.47e-06,0.6133\n",
      "4.79e-06,0.6287\n",
      "5.13e-06,0.6245\n",
      "5.50e-06,0.6202\n",
      "5.89e-06,0.6276\n",
      "6.31e-06,0.6497\n",
      "6.76e-06,0.6444\n",
      "7.24e-06,0.6563\n",
      "7.76e-06,0.6522\n",
      "8.32e-06,0.6445\n",
      "8.91e-06,0.6528\n",
      "9.55e-06,0.6492\n",
      "1.02e-05,0.6454\n",
      "1.10e-05,0.6377\n",
      "1.17e-05,0.6327\n",
      "1.26e-05,0.6375\n",
      "1.35e-05,0.6366\n",
      "1.45e-05,0.6414\n",
      "1.55e-05,0.6292\n",
      "1.66e-05,0.6309\n",
      "1.78e-05,0.6420\n",
      "1.91e-05,0.6435\n",
      "2.04e-05,0.6336\n",
      "2.19e-05,0.6288\n",
      "2.34e-05,0.6316\n",
      "2.51e-05,0.6183\n",
      "2.69e-05,0.6271\n",
      "2.88e-05,0.6256\n",
      "3.09e-05,0.6302\n",
      "3.31e-05,0.6296\n",
      "3.55e-05,0.6256\n",
      "3.80e-05,0.6303\n",
      "4.07e-05,0.6290\n",
      "4.37e-05,0.6216\n",
      "4.68e-05,0.6484\n",
      "5.01e-05,0.6590\n",
      "5.37e-05,0.6888\n",
      "5.75e-05,0.6873\n",
      "6.17e-05,0.6989\n",
      "6.61e-05,0.7213\n",
      "7.08e-05,0.7312\n",
      "7.59e-05,0.7293\n",
      "8.13e-05,0.7280\n",
      "8.71e-05,0.7344\n",
      "9.33e-05,0.7284\n",
      "1.00e-04,0.7559\n",
      "1.07e-04,0.7666\n"
     ]
    }
   ],
   "source": [
    "new_data = data\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 3\n",
    "LEARNING_RATE = 1e-7\n",
    "STEP = 100\n",
    "SKIP = 1\n",
    "smooth = 0.05\n",
    "\n",
    "# Freeze BERT\n",
    "model.unfreeze()\n",
    "\n",
    "\"\"\" TRAINING \"\"\"\n",
    "dataset = BonzDataset(data.iloc[:,-3:], None)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "''' STORING WHILE TRAINING'''\n",
    "lr_list = []\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "'''\n",
    "for i in range(3):\n",
    "    # Setup scheduler each period\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  base_lr=0, \n",
    "                                                  max_lr=LEARNING_RATE*10, \n",
    "                                                  step_size_up=EPOCH, \n",
    "                                                  cycle_momentum=False)\n",
    "    scheduler.step()\n",
    "    \n",
    "    for epoch in tqdm.notebook.trange(EPOCH):\n",
    "\n",
    "        # Load original weights\n",
    "        model.load_state_dict(origin_sd) \n",
    "        loss_train = 0\n",
    "\n",
    "        for idx, (a, b, c) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            predict, loss = model(a.to(DEVICE), \n",
    "                                  b.to(DEVICE), \n",
    "                                  c.to(DEVICE), \n",
    "                                  weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "            loss.backward()\n",
    "            loss_train += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "\n",
    "        # Store metrics\n",
    "        lr_list.append(current_lr)\n",
    "        loss_list.append(loss_train)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    LEARNING_RATE *= 10\n",
    "'''\n",
    "\n",
    "'''\n",
    "lr_lambda = lambda x: math.exp(x * math.log(LEARNING_RATE*1e4 / LEARNING_RATE) / (STEP-1))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "for epoch in tqdm.notebook.trange(EPOCH*4):\n",
    "    # Load original weights\n",
    "    loss_train = 0\n",
    "\n",
    "    for idx, (a, b, c) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predict, loss = model(a.to(DEVICE), \n",
    "                              b.to(DEVICE), \n",
    "                              c.to(DEVICE), \n",
    "                              weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "        loss.backward()\n",
    "        loss_train += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "    print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "\n",
    "    # Store metrics\n",
    "    lr_list.append(current_lr)\n",
    "    loss_list.append(loss_train)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "'''\n",
    "\n",
    "model.load_state_dict(torch.load('freeze.pth')) \n",
    "\n",
    "lr_lambda = lambda x: math.exp(x * math.log(LEARNING_RATE*1e3 / LEARNING_RATE) / (STEP))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "for idx, (a, b, c) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    predict, loss = model(a.to(DEVICE), \n",
    "                          b.to(DEVICE), \n",
    "                          c.to(DEVICE), \n",
    "                          weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    # Update learning rate\n",
    "    if idx >= SKIP:\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_list.append(current_lr)\n",
    "        \n",
    "        if idx == SKIP:\n",
    "            loss_list.append(loss.item())\n",
    "        elif idx > SKIP:\n",
    "            temp = loss.item()*smooth + (1-smooth)*loss_list[-1]\n",
    "            loss_list.append(temp)\n",
    "    \n",
    "        # Stop when reach the step\n",
    "        if idx >= (STEP+SKIP):\n",
    "            break\n",
    "    \n",
    "# Print losss per learning rate\n",
    "for a, b in zip(lr_list, loss_list):\n",
    "    print(f'{a:.2e},{b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with FREEZE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e65c979809d4cf9b22615f42ed99930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 13939.86, Learning Rate = 3.50e-04\n",
      "Epoch: 1, Loss = 14089.19, Learning Rate = 3.50e-04\n",
      "Epoch: 2, Loss = 13919.82, Learning Rate = 5.00e-05\n",
      "Epoch: 3, Loss = 13891.37, Learning Rate = 3.50e-04\n",
      "Epoch: 4, Loss = 14064.57, Learning Rate = 3.50e-04\n",
      "Epoch: 5, Loss = 13918.16, Learning Rate = 5.01e-05\n",
      "Epoch: 6, Loss = 13869.50, Learning Rate = 3.50e-04\n",
      "Epoch: 7, Loss = 14059.37, Learning Rate = 3.50e-04\n",
      "Epoch: 8, Loss = 13893.49, Learning Rate = 5.01e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 7\n",
    "EPOCH = 9\n",
    "BASE_LR = 5e-5\n",
    "MAX_LR = 5e-4\n",
    "CYCLE = 3\n",
    "\n",
    "# Freeze BERT\n",
    "model.freeze()\n",
    "\n",
    "\"\"\" TRAINING \"\"\"\n",
    "dataset = BonzDataset(data.iloc[:,-3:], None)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model.load_state_dict(torch.load('origin_sd.pth'))\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              base_lr=BASE_LR, \n",
    "                                              max_lr=MAX_LR, \n",
    "                                              step_size_up=(dataset.__len__()*EPOCH/BATCH_SIZE) // (CYCLE*2), \n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm.notebook.trange(EPOCH):\n",
    "    # Load original weights\n",
    "    loss_train = 0\n",
    "\n",
    "    for idx, (a, b, c) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predict, loss = model(a.to(DEVICE), \n",
    "                              b.to(DEVICE), \n",
    "                              c.to(DEVICE), \n",
    "                              weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "        loss.backward()\n",
    "        loss_train += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "    print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "\n",
    "# SAVE AND LOAD PRE-TRAINED LLR EMBEDDING\n",
    "torch.save(model.state_dict(), 'freeze.pth')\n",
    "origin_sd = torch.load('freeze.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with K-FOLD and UNFREEZE BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c7e4746e974410ac9bf168cc8b0490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a554d05a95d5456a9c8e58ce59615c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12482.23, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11694.65, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11016.31, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12add6969a72484282e41674ad04b1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12519.23, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11700.66, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11071.92, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c0bdc2af044887815674f942873d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12501.33, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11718.18, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11046.63, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a139cd6fd1f4165ae1fd8a609527a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12502.21, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11727.24, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11093.66, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557a6752e3594e45b00080888a840a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12600.18, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11795.26, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11170.70, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ef56a4a06e4695ae4a9e7280e82930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12516.23, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11736.89, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11077.52, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c936bd6119784d7e8f97a165265bffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12523.42, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11768.38, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11116.14, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fed25c9a714cf0a7bb6649e39e4908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12474.36, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11656.35, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11009.30, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e5380dc1594115ada7043af725f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12476.58, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11639.35, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 10973.73, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73d0ba491df48bd9963de06b4339619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 12481.16, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 11704.76, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 11032.38, Learning Rate = 2.00e-05\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training with K-fold\n",
    "new_data = data.sample(frac=1).reset_index(drop=True)\n",
    "#new_data = data\n",
    "kf = KFold(10)\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 3\n",
    "BASE_LR = 2e-5\n",
    "MAX_LR = 5e-5\n",
    "CYCLE = 2\n",
    "\n",
    "# Unfreeze BERT\n",
    "model.unfreeze()\n",
    "\n",
    "last_predict = []\n",
    "i = 0\n",
    "for train_idx, test_idx in tqdm.notebook.tqdm(kf.split(new_data)):\n",
    "    train_data = new_data.iloc[train_idx]\n",
    "    test_data = new_data.iloc[test_idx]\n",
    "    \n",
    "    model.load_state_dict(torch.load('./state_dict/freeze.pth'))\n",
    "    \"\"\" TRAINING \"\"\"\n",
    "    dataset = BonzDataset(train_data.iloc[:,-3:], None)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR)\n",
    "    '''\n",
    "    try:\n",
    "        scheduler.load_state_dict(torch.load('./state_dict/scheduler_sd.pth'))\n",
    "        print('Load scheduler')\n",
    "    except:\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                      base_lr=BASE_LR, \n",
    "                                                      max_lr=MAX_LR, \n",
    "                                                      step_size_up=(dataset.__len__()*EPOCH/BATCH_SIZE) // (CYCLE*2), \n",
    "                                                      cycle_momentum=False)\n",
    "        torch.save(scheduler.state_dict(), 'scheduler_sd.pth')\n",
    "        print('Create scheduler')\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  base_lr=BASE_LR, \n",
    "                                                  max_lr=MAX_LR, \n",
    "                                                  step_size_up=(dataset.__len__()*EPOCH/BATCH_SIZE) // (CYCLE*2), \n",
    "                                                  cycle_momentum=False)\n",
    "    print(torch.cuda.memory_summary())   \n",
    "    time.sleep(2)\n",
    "    '''\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm.notebook.trange(EPOCH):\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        loss_train = 0\n",
    "        for a, b, c in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predict, loss = model(a.to(DEVICE), \n",
    "                                  b.to(DEVICE), \n",
    "                                  c.to(DEVICE), \n",
    "                                  weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "            #predict, loss = model(a.to(DEVICE), None, c.to(DEVICE))[:2]   # This is normal BERT\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "                \n",
    "        \n",
    "    ''' TESTING  ''' \n",
    "    model.eval()\n",
    "    dataset = BonzDataset(test_data.iloc[:,-3:], None)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    for a, b, c in dataloader:\n",
    "        with torch.no_grad():\n",
    "            predict = model(a.to(DEVICE), b.to(DEVICE))[0] # This is L-BERT\n",
    "            #predict = model(a.to(DEVICE), None)[0] # This is normal BERT\n",
    "        last_predict.extend(predict.detach().cpu().numpy().tolist())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN FULL DATA TO PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9704efebdac497db02038a05f1dc6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss = 13855.07, Learning Rate = 2.00e-05\n",
      "Epoch: 1, Loss = 13024.75, Learning Rate = 2.00e-05\n",
      "Epoch: 2, Loss = 12348.88, Learning Rate = 2.00e-05\n",
      "Epoch: 3, Loss = 11501.65, Learning Rate = 2.00e-05\n",
      "Epoch: 4, Loss = 10414.48, Learning Rate = 2.00e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training with K-fold\n",
    "#new_data = data.sample(frac=1).reset_index(drop=True)\n",
    "new_data = data\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 5\n",
    "BASE_LR = 2e-5\n",
    "\n",
    "# Unfreeze BERT\n",
    "model.unfreeze()\n",
    "\n",
    "last_predict = []\n",
    "\n",
    "model.load_state_dict(torch.load('./state_dict/freeze.pth'))\n",
    "\"\"\" TRAINING \"\"\"\n",
    "dataset = BonzDataset(new_data.iloc[:,-3:], None)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR)\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm.notebook.trange(EPOCH):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    loss_train = 0\n",
    "    for a, b, c in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predict, loss = model(a.to(DEVICE), \n",
    "                              b.to(DEVICE), \n",
    "                              c.to(DEVICE), \n",
    "                              weight_loss=weight_loss)[:2]   # This is L-BERT\n",
    "        #predict, loss = model(a.to(DEVICE), None, c.to(DEVICE))[:2]   # This is normal BERT\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "    current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "    print(f'Epoch: {epoch}, Loss = {loss_train:.2f}, Learning Rate = {current_lr:.2e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55     41143\n",
      "           1       0.82      0.89      0.85    109509\n",
      "           2       0.28      0.24      0.25      1922\n",
      "\n",
      "    accuracy                           0.77    152574\n",
      "   macro avg       0.57      0.54      0.55    152574\n",
      "weighted avg       0.76      0.77      0.77    152574\n",
      "\n",
      "SIT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.59     41886\n",
      "           1       0.83      0.90      0.87    109017\n",
      "           2       0.28      0.27      0.27      1671\n",
      "\n",
      "    accuracy                           0.79    152574\n",
      "   macro avg       0.59      0.57      0.58    152574\n",
      "weighted avg       0.78      0.79      0.78    152574\n",
      "\n",
      "ENT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56     44245\n",
      "           1       0.81      0.86      0.83     93024\n",
      "           2       0.45      0.34      0.38     15305\n",
      "\n",
      "    accuracy                           0.72    152574\n",
      "   macro avg       0.61      0.58      0.59    152574\n",
      "weighted avg       0.70      0.72      0.71    152574\n",
      "\n",
      "CUS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74     24753\n",
      "           1       0.93      0.96      0.95    125874\n",
      "           2       0.36      0.01      0.02      1947\n",
      "\n",
      "    accuracy                           0.91    152574\n",
      "   macro avg       0.68      0.56      0.57    152574\n",
      "weighted avg       0.90      0.91      0.90    152574\n",
      "\n",
      "VOM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.58      0.65     31240\n",
      "           1       0.87      0.95      0.91    116713\n",
      "           2       0.38      0.05      0.09      4621\n",
      "\n",
      "    accuracy                           0.85    152574\n",
      "   macro avg       0.66      0.53      0.55    152574\n",
      "weighted avg       0.83      0.85      0.83    152574\n",
      "\n",
      "CLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.45      0.47     15820\n",
      "           1       0.78      0.93      0.85    106232\n",
      "           2       0.61      0.22      0.32     30522\n",
      "\n",
      "    accuracy                           0.74    152574\n",
      "   macro avg       0.62      0.53      0.54    152574\n",
      "weighted avg       0.71      0.74      0.70    152574\n",
      "\n",
      "CKI:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51     17618\n",
      "           1       0.77      0.92      0.84    104638\n",
      "           2       0.60      0.21      0.32     30318\n",
      "\n",
      "    accuracy                           0.73    152574\n",
      "   macro avg       0.63      0.55      0.56    152574\n",
      "weighted avg       0.71      0.73      0.70    152574\n",
      "\n",
      "FNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58     38613\n",
      "           1       0.68      0.84      0.75     80718\n",
      "           2       0.58      0.23      0.33     33243\n",
      "\n",
      "    accuracy                           0.64    152574\n",
      "   macro avg       0.61      0.55      0.55    152574\n",
      "weighted avg       0.63      0.64      0.61    152574\n",
      "\n",
      "LEG, 57.28,    54.03,    55.35,    77.36\n",
      "SIT, 59.38,    56.59,    57.66,    79.27\n",
      "ENT, 61.18,    58.16,    59.35,    71.81\n",
      "CUS, 68.29,    56.07,    56.58,    90.67\n",
      "VOM, 66.33,    52.81,    54.94,    84.87\n",
      "CLE, 62.47,    53.06,    54.42,    73.65\n",
      "CKI, 63.02,    54.72,    55.60,    73.29\n",
      "FNB, 61.19,    55.19,    55.31,    64.24\n"
     ]
    }
   ],
   "source": [
    "last_predict_ = torch.tensor(last_predict)\n",
    "last_predict_ = torch.softmax(last_predict_, 1)\n",
    "y_predict = torch.argmax(last_predict_, 1)\n",
    "y_true = np.asarray(list(new_data.labels))\n",
    "\n",
    "\n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}:\\n{classification_report(y_true[:,i], y_predict[:,i])}')\n",
    "    \n",
    "for i, asp in enumerate(ASPECT_NAMES):\n",
    "    print(f'{asp}, {precision_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {accuracy_score(y_true[:,i], y_predict[:,i])*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(y_predict, 'result/LCAT_5epoch_1e5_WeightClass_xavier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT NO FILLING IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AirlineName</th>\n",
       "      <th>ReviewDate</th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>With everyone trying to get home in the Covid ...</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>Ad a lot of people did, we had to scramble to ...</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>After coming into Changi airport and worrying ...</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>We booked to fly from Heathrow to Newark. The ...</td>\n",
       "      <td>[Legroom:2, Seat comfort:2, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>Love Virgin, great staff, food good, quality o...</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>Virgin upper class is outstanding, really very...</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>Virgins premium economy is the best I have com...</td>\n",
       "      <td>[Legroom:3, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>At my age I cannot face an overnight in econom...</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190986 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AirlineName ReviewDate  \\\n",
       "0            Singapore Airlines   Sep 2020   \n",
       "1            Singapore Airlines   Sep 2020   \n",
       "2            Singapore Airlines   Sep 2020   \n",
       "3            Singapore Airlines   Sep 2020   \n",
       "4            Singapore Airlines   Sep 2020   \n",
       "...                         ...        ...   \n",
       "190981  Virgin Atlantic Airways   Jan 2016   \n",
       "190982  Virgin Atlantic Airways   Jan 2016   \n",
       "190983  Virgin Atlantic Airways   Jan 2016   \n",
       "190984  Virgin Atlantic Airways   Jan 2016   \n",
       "190985  Virgin Atlantic Airways   Jan 2016   \n",
       "\n",
       "                                               ReviewText  \\\n",
       "0       With everyone trying to get home in the Covid ...   \n",
       "1       Ad a lot of people did, we had to scramble to ...   \n",
       "2       After coming into Changi airport and worrying ...   \n",
       "3       Great service, great plane, great pricing. We ...   \n",
       "4       My husband and I were to fly home from Houston...   \n",
       "...                                                   ...   \n",
       "190981  We booked to fly from Heathrow to Newark. The ...   \n",
       "190982  Love Virgin, great staff, food good, quality o...   \n",
       "190983  Virgin upper class is outstanding, really very...   \n",
       "190984  Virgins premium economy is the best I have com...   \n",
       "190985  At my age I cannot face an overnight in econom...   \n",
       "\n",
       "                                                  Aspects  Predicted  LEG  \\\n",
       "0       [Legroom:4, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "1       [Legroom:4, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "2       [Legroom:4, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "3       [Legroom:4, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "4       [Legroom:5, Seat comfort:5, In-flight Entertai...      False    1   \n",
       "...                                                   ...        ...  ...   \n",
       "190981  [Legroom:2, Seat comfort:2, In-flight Entertai...      False    0   \n",
       "190982  [Legroom:5, Seat comfort:5, In-flight Entertai...      False    1   \n",
       "190983  [Legroom:5, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "190984  [Legroom:3, Seat comfort:5, In-flight Entertai...      False    0   \n",
       "190985  [Legroom:5, Seat comfort:4, In-flight Entertai...      False    1   \n",
       "\n",
       "        SIT  ENT  CUS  VOM  CLE  CKI  FNB  \\\n",
       "0         1    1    1    1    1    1    1   \n",
       "1         1    1    1    1    1    1    1   \n",
       "2         1    1    1    1    1    1    1   \n",
       "3         1    1    1    1    1    1    1   \n",
       "4         1    1    0    1    1    1    1   \n",
       "...     ...  ...  ...  ...  ...  ...  ...   \n",
       "190981    0    0    0    0    2    2    2   \n",
       "190982    1    1    1    1    2    2    2   \n",
       "190983    1    1    1    1    2    2    2   \n",
       "190984    1    1    0    0    2    2    2   \n",
       "190985    1    0    0    0    2    2    2   \n",
       "\n",
       "                                                input_ids  \n",
       "0       [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...  \n",
       "1       [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...  \n",
       "2       [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...  \n",
       "3       [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...  \n",
       "4       [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...  \n",
       "...                                                   ...  \n",
       "190981  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...  \n",
       "190982  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...  \n",
       "190983  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...  \n",
       "190984  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...  \n",
       "190985  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...  \n",
       "\n",
       "[190986 rows x 14 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv('./data//data_v3.txt', sep='\\t', header=None, names=COL_NAMES)\n",
    "full_data = full_data[['AirlineName', 'ReviewDate', 'ReviewText', 'Aspects']]\n",
    "full_data['Predicted'] = (full_data['Aspects'] == 'No filling in')\n",
    "full_data.Aspects = full_data.Aspects.str.split('|').values\n",
    "\n",
    "'''Split aspects to new columns'''\n",
    "aspects_splitted = split_aspect(full_data.Aspects.values)\n",
    "for i in range(len(ASPECT_NAMES)):\n",
    "    full_data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "\n",
    "full_data['input_ids'], tokenizer = tokenize_data(full_data.ReviewText.values)\n",
    "\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AirlineName</th>\n",
       "      <th>ReviewDate</th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>I cant rate Singspore Air any higher for their...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 1045, 2064, 2102, 3446, 10955, 26691, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>They told me it only take 4-6 weeks for the am...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2027, 2409, 2033, 2009, 2069, 2202, 1018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>This is a lovely airline, the stewardesses in ...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2023, 2003, 1037, 8403, 8582, 1010, 1996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>We were due to fly to Australia in October. We...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 2020, 2349, 2000, 4875, 2000, 2660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>Our flights were cancelled and 3 months later ...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2256, 7599, 2020, 8014, 1998, 1017, 2706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190862</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Feb 2016</td>\n",
       "      <td>Boarding at the airport was rather disorganise...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 9405, 2012, 1996, 3199, 2001, 2738, 4487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190890</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Feb 2016</td>\n",
       "      <td>My wife and I recently booked a return flight ...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2026, 2564, 1998, 1045, 3728, 17414, 103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190895</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Feb 2016</td>\n",
       "      <td>Arriving at the Upper Class Wing to the lounge...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 7194, 2012, 1996, 3356, 2465, 3358, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190949</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Feb 2016</td>\n",
       "      <td>We booked our flights for a family holiday for...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2256, 7599, 2005, 1037, 215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190970</th>\n",
       "      <td>Virgin Atlantic Airways</td>\n",
       "      <td>Jan 2016</td>\n",
       "      <td>Virgin Atlantic best airline to travel, cabin ...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 4448, 2190, 8582, 2000, 3604, 1010...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38412 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AirlineName ReviewDate  \\\n",
       "5            Singapore Airlines   Aug 2020   \n",
       "6            Singapore Airlines   Aug 2020   \n",
       "7            Singapore Airlines   Aug 2020   \n",
       "9            Singapore Airlines   Aug 2020   \n",
       "12           Singapore Airlines   Aug 2020   \n",
       "...                         ...        ...   \n",
       "190862  Virgin Atlantic Airways   Feb 2016   \n",
       "190890  Virgin Atlantic Airways   Feb 2016   \n",
       "190895  Virgin Atlantic Airways   Feb 2016   \n",
       "190949  Virgin Atlantic Airways   Feb 2016   \n",
       "190970  Virgin Atlantic Airways   Jan 2016   \n",
       "\n",
       "                                               ReviewText          Aspects  \\\n",
       "5       I cant rate Singspore Air any higher for their...  [No filling in]   \n",
       "6       They told me it only take 4-6 weeks for the am...  [No filling in]   \n",
       "7       This is a lovely airline, the stewardesses in ...  [No filling in]   \n",
       "9       We were due to fly to Australia in October. We...  [No filling in]   \n",
       "12      Our flights were cancelled and 3 months later ...  [No filling in]   \n",
       "...                                                   ...              ...   \n",
       "190862  Boarding at the airport was rather disorganise...  [No filling in]   \n",
       "190890  My wife and I recently booked a return flight ...  [No filling in]   \n",
       "190895  Arriving at the Upper Class Wing to the lounge...  [No filling in]   \n",
       "190949  We booked our flights for a family holiday for...  [No filling in]   \n",
       "190970  Virgin Atlantic best airline to travel, cabin ...  [No filling in]   \n",
       "\n",
       "        Predicted  LEG  SIT  ENT  CUS  VOM  CLE  CKI  FNB  \\\n",
       "5            True    2    2    2    2    2    2    2    2   \n",
       "6            True    2    2    2    2    2    2    2    2   \n",
       "7            True    2    2    2    2    2    2    2    2   \n",
       "9            True    2    2    2    2    2    2    2    2   \n",
       "12           True    2    2    2    2    2    2    2    2   \n",
       "...           ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "190862       True    2    2    2    2    2    2    2    2   \n",
       "190890       True    2    2    2    2    2    2    2    2   \n",
       "190895       True    2    2    2    2    2    2    2    2   \n",
       "190949       True    2    2    2    2    2    2    2    2   \n",
       "190970       True    2    2    2    2    2    2    2    2   \n",
       "\n",
       "                                                input_ids  \n",
       "5       [101, 1045, 2064, 2102, 3446, 10955, 26691, 22...  \n",
       "6       [101, 2027, 2409, 2033, 2009, 2069, 2202, 1018...  \n",
       "7       [101, 2023, 2003, 1037, 8403, 8582, 1010, 1996...  \n",
       "9       [101, 2057, 2020, 2349, 2000, 4875, 2000, 2660...  \n",
       "12      [101, 2256, 7599, 2020, 8014, 1998, 1017, 2706...  \n",
       "...                                                   ...  \n",
       "190862  [101, 9405, 2012, 1996, 3199, 2001, 2738, 4487...  \n",
       "190890  [101, 2026, 2564, 1998, 1045, 3728, 17414, 103...  \n",
       "190895  [101, 7194, 2012, 1996, 3356, 2465, 3358, 2000...  \n",
       "190949  [101, 2057, 17414, 2256, 7599, 2005, 1037, 215...  \n",
       "190970  [101, 6261, 4448, 2190, 8582, 2000, 3604, 1010...  \n",
       "\n",
       "[38412 rows x 14 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_data = full_data[full_data['Predicted'] == True]\n",
    "predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09d9e9953664edf8a44db8087c0b264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Calculate LLR scores', max=8.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659057ba24fd4a9f9b4bb3a949bfa85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate top LLR words', max=8.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ee0e1eed83441f9c8c0ac3853132c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38412.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Stopwords in English\n",
    "stopwords_ids = tokenizer.convert_tokens_to_ids(stopwords.words('english'))\n",
    "\n",
    "llr_scores = {}\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Calculate LLR scores'):\n",
    "    llr_df = generate_llr_score(data, aspect)\n",
    "    \n",
    "    # Clear stopword ids\n",
    "    llr_df = llr_df.drop(stopwords_ids, 0)\n",
    "    \n",
    "    llr_scores[aspect] = llr_df\n",
    "\n",
    "\n",
    "llr_words = dict()\n",
    "\n",
    "for aspect in tqdm.notebook.tqdm(ASPECT_NAMES, desc='Generate top LLR words'):\n",
    "    kw_label = dict()\n",
    "    for class_ in [0,1,2]:\n",
    "        # Sort keywords based on aspect, class and top_n words\n",
    "        kw_list = list(llr_scores[aspect][class_].sort_values(ascending=False)[:TOPN].index)\n",
    "        \n",
    "        kw_label[class_] = kw_list\n",
    "        \n",
    "    llr_words[aspect] = kw_label\n",
    "\n",
    "llr_embedding_list = []\n",
    "\n",
    "for idx in tqdm.notebook.tqdm(predicted_data.index):\n",
    "    tokens = predicted_data.input_ids[idx]\n",
    "    \n",
    "    llr_embedding = []\n",
    "    for aspect in ASPECT_NAMES:\n",
    "        temp = [3] * tokens.shape[0]\n",
    "        for j in range(tokens.shape[0]):\n",
    "            for class_, wordlist in llr_words[aspect].items():\n",
    "                if tokens[j] in wordlist:\n",
    "                    temp[j] = class_\n",
    "                    break\n",
    "        llr_embedding.append(temp)\n",
    "    \n",
    "    llr_embedding_list.append(llr_embedding)\n",
    "\n",
    "predicted_data['llr_embeddings'] = llr_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataset = BonzDataset(predicted_data.iloc[:,-3:], None)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "last_predict = []\n",
    "\n",
    "for a, b in dataloader:\n",
    "    with torch.no_grad():\n",
    "        predict = model(a.to(DEVICE), b.to(DEVICE))[0] # This is L-BERT\n",
    "        #predict = model(a.to(DEVICE), None)[0] # This is normal BERT\n",
    "    last_predict.extend(predict.detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38412, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor(last_predict)\n",
    "pred = torch.softmax(pred, 1)\n",
    "pred = torch.argmax(pred, 1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, asp in enumerate(ASPECT_NAMES):\n",
    "    full_data.loc[predicted_data.index, asp] = pred[:,idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AirlineName</th>\n",
       "      <th>ReviewDate</th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Sep 2020</td>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>I cant rate Singspore Air any higher for their...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1045, 2064, 2102, 3446, 10955, 26691, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>They told me it only take 4-6 weeks for the am...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2027, 2409, 2033, 2009, 2069, 2202, 1018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>This is a lovely airline, the stewardesses in ...</td>\n",
       "      <td>[No filling in]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2023, 2003, 1037, 8403, 8582, 1010, 1996...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AirlineName ReviewDate  \\\n",
       "3  Singapore Airlines   Sep 2020   \n",
       "4  Singapore Airlines   Sep 2020   \n",
       "5  Singapore Airlines   Aug 2020   \n",
       "6  Singapore Airlines   Aug 2020   \n",
       "7  Singapore Airlines   Aug 2020   \n",
       "\n",
       "                                          ReviewText  \\\n",
       "3  Great service, great plane, great pricing. We ...   \n",
       "4  My husband and I were to fly home from Houston...   \n",
       "5  I cant rate Singspore Air any higher for their...   \n",
       "6  They told me it only take 4-6 weeks for the am...   \n",
       "7  This is a lovely airline, the stewardesses in ...   \n",
       "\n",
       "                                             Aspects  Predicted  LEG  SIT  \\\n",
       "3  [Legroom:4, Seat comfort:4, In-flight Entertai...      False    1    1   \n",
       "4  [Legroom:5, Seat comfort:5, In-flight Entertai...      False    1    1   \n",
       "5                                    [No filling in]       True    1    1   \n",
       "6                                    [No filling in]       True    0    0   \n",
       "7                                    [No filling in]       True    1    1   \n",
       "\n",
       "   ENT  CUS  VOM  CLE  CKI  FNB  \\\n",
       "3    1    1    1    1    1    1   \n",
       "4    1    0    1    1    1    1   \n",
       "5    1    1    1    1    1    1   \n",
       "6    0    0    0    0    0    0   \n",
       "7    1    1    1    1    1    1   \n",
       "\n",
       "                                           input_ids  \n",
       "3  [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...  \n",
       "4  [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...  \n",
       "5  [101, 1045, 2064, 2102, 3446, 10955, 26691, 22...  \n",
       "6  [101, 2027, 2409, 2033, 2009, 2069, 2202, 1018...  \n",
       "7  [101, 2023, 2003, 1037, 8403, 8582, 1010, 1996...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.loc[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.iloc[:,:-1].to_csv('./data/full_data.txt', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
