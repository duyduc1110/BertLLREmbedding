{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/udacity-pytorch-challengers/ideas-on-how-to-fine-tune-a-pre-trained-model-in-pytorch-184c47185a20\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertTokenizerFast, BertModel, AdamW, TFBertModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_bert import BertEmbeddings, BertSelfAttention\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from apex import amp, optimizers\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "COL_NAMES = ['TopNumber', 'AirlineName','ReviewerName','Rating','ReviewDate','ReviewTitle',\\\n",
    "             'ReviewText','Tags', 'DateofTravel', 'Aspects', 'ResponserName', 'ResponseDate', 'ResponseText', 'ReviewerProfileUrl',\\\n",
    "             'AirlineNation', 'CrawlTime']\n",
    "\n",
    "PRE_TRAINED = 'bert-base-uncased'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ASPECT_NAMES = ['LEG', 'SIT', 'ENT', 'CUS', 'VOM', 'CLE', 'CKI', 'FNB']\n",
    "VOCAB_DIC = BertTokenizerFast.from_pretrained(PRE_TRAINED).get_vocab()\n",
    "TOPN = 50\n",
    "\n",
    "\n",
    "        \n",
    "# This one is implemented with weight loss per class            \n",
    "class BertBonzWeightLoss(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertBonzWeightLoss, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.embeddings.llr_embeddings = nn.ModuleList(nn.Embedding(4, 768, 3) for _ in range(len(ASPECT_NAMES)))\n",
    "        self.classifier = nn.Linear(768, config.num_aspect*3)\n",
    "        self.init_weights()\n",
    "        self.embeddings.llr_embeddings.apply(self._xavier)\n",
    "        self.pooler.apply(self._xavier)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                llr_ids=None, \n",
    "                labels=None, \n",
    "                token_type_ids=None, \n",
    "                position_ids=None,\n",
    "                weight_loss=None):\n",
    "        # BERT EMBEDDINGS NEW\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        inputs_embeds = self.embeddings.word_embeddings(input_ids)\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        if llr_ids is not None:\n",
    "            temp = [self.embeddings.llr_embeddings[i](llr_ids[:,i,:]) for i in range(self.config.num_aspect)]\n",
    "            llr_embeddings = sum(temp)\n",
    "        else:\n",
    "            llr_embeddings = torch.zeros(inputs_embeds.size(), device=device).fill_(3).long()\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + llr_embeddings\n",
    "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "        embeddings = self.embeddings.dropout(embeddings)\n",
    "        \n",
    "        \n",
    "        # BERT ENCODER\n",
    "        encoder_outputs = self.encoder(\n",
    "            embeddings,\n",
    "            attention_mask=None,\n",
    "            head_mask=[None]*12,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            output_attentions=self.config.output_attentions\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # CLASSIFIER\n",
    "        CLS_token = sequence_output[:,0]\n",
    "        predict = self.classifier(CLS_token)\n",
    "        \n",
    "        loss_fn = nn.functional.cross_entropy\n",
    "        if labels is not None:\n",
    "            if weight_loss is None:\n",
    "                loss = loss_fn(predict.view(input_shape[0], 3,-1), labels)\n",
    "            else:\n",
    "                loss = torch.tensor(0).float().to(DEVICE)\n",
    "                for asp_i in range(len(ASPECT_NAMES)):\n",
    "                    loss += loss_fn(predict.view(input_shape[0], 3,-1)[:,:,asp_i], labels[:,asp_i], weight_loss[asp_i, :])\n",
    "                loss /= len(ASPECT_NAMES)\n",
    "                    \n",
    "            outputs = (predict.view(input_shape[0], 3,-1), loss, CLS_token, sequence_output) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "        else:\n",
    "            outputs = (predict.view(input_shape[0], 3,-1), CLS_token, sequence_output) + encoder_outputs[1:]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def load_pretrained_weight(self):\n",
    "        sd = self.state_dict()\n",
    "        sd_bert_pretrained = BertModel.from_pretrained(PRE_TRAINED).state_dict()\n",
    "        for k in sd_bert_pretrained.keys():\n",
    "            if k in sd.keys():\n",
    "                sd[k] = sd_bert_pretrained[k]\n",
    "        self.load_state_dict(sd)\n",
    "        print('Succesfully load pre-trained weights')\n",
    "        \n",
    "    def llr_embed_pad(self):\n",
    "        for i in range(len(ASPECT_NAMES)):\n",
    "            temp = self.embeddings.llr_embeddings[i].weight.data\n",
    "            temp[-1,:] = torch.zeros(temp.size(1))\n",
    "            \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    "    def unfreeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "                \n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.embeddings.llr_embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "\n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data, llr_words):\n",
    "        self.input_ids = torch.LongTensor(list(data.input_ids))\n",
    "        self.llr_embeddings = torch.LongTensor(list(data.llr_embeddings))\n",
    "        if 'labels' in data.columns:\n",
    "            self.labels = torch.LongTensor(list(data.labels))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        self.llr_words = llr_words\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        tokens = self.data.input_ids[idx]\n",
    "        \n",
    "        llr_embedding = []\n",
    "        for aspect in ASPECT_NAMES:\n",
    "            temp = [3] * tokens.shape[0]\n",
    "            for j in range(tokens.shape[0]):\n",
    "                for class_, wordlist in llr_words[aspect].items():\n",
    "                    if tokens[j] in wordlist:\n",
    "                        temp[j] = class_\n",
    "                        break\n",
    "            llr_embedding.append(temp)\n",
    "        \n",
    "        llr_embedding = torch.stack([torch.LongTensor(i) for i in llr_embedding], 0)\n",
    "        \n",
    "        \n",
    "        outputs = (torch.LongTensor(tokens), llr_embedding)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            outputs = (torch.LongTensor(tokens), llr_embedding, torch.LongTensor(self.data.labels[idx]))\n",
    "        '''\n",
    "        if self.labels is None:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx])\n",
    "        else:\n",
    "            outputs = (self.input_ids[idx], self.llr_embeddings[idx], self.labels[idx])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    \n",
    "def split_aspect(data):\n",
    "    temp = np.full((8, data.shape[0]), 2, np.int)\n",
    "    for idx in range(data.shape[0]):\n",
    "        aspect = data[idx]\n",
    "        for i, asp in enumerate(['Legroom', 'Seat', 'Entertainment', 'Customer', 'Value', 'Cleanliness', 'Check-in', 'Food']):\n",
    "            for sub_asp in aspect:\n",
    "                if asp in sub_asp:\n",
    "                    pol = int(sub_asp[-1])\n",
    "                    temp[i, idx] = 1 if pol > 3 else 0\n",
    "                    break\n",
    "    return temp\n",
    "            \n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED)\n",
    "    input_ids = tokenizer(list(data))['input_ids']\n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, padding='post', truncating='post')\n",
    "    \n",
    "    return (list(input_ids), tokenizer)\n",
    "    \n",
    "    \n",
    "def get_data(FILE_PATH, COL_NAMES):\n",
    "    raw_data = pd.read_csv(FILE_PATH, sep='\\t', header=None, names=COL_NAMES)\n",
    "    data = raw_data[['ReviewText', 'Rating', 'Aspects']]\n",
    "    data = data[data['Aspects'] != 'No filling in'] # Filter none aspects\n",
    "    data.Aspects = data.Aspects.str.split('|').values\n",
    "    \n",
    "    '''Split aspects to new columns'''\n",
    "    aspects_splitted = split_aspect(data.Aspects.values)\n",
    "    for i in range(len(ASPECT_NAMES)):\n",
    "        data[ASPECT_NAMES[i]] = aspects_splitted[i,:]\n",
    "        \n",
    "    data['input_ids'], tokenizer = tokenize_data(data.ReviewText.values) # Generate input_ids from review text\n",
    "    \n",
    "    return data, tokenizer\n",
    "\n",
    "\n",
    "def word_class_freq(data, aspect_name, aspect_class=3):\n",
    "    temp = np.zeros((33000, aspect_class), np.int)\n",
    "    ids = data.input_ids.values\n",
    "    labels = data[aspect_name].values\n",
    "\n",
    "    for sub_ids, sub_lb in zip(ids, labels):\n",
    "        set_ids = set(sub_ids)\n",
    "        for ids in set_ids:\n",
    "            temp[ids, sub_lb] += 1\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def calculate_llr(temp_df, labels):\n",
    "    N = data.shape[0]\n",
    "    total_scores = []\n",
    "\n",
    "    for i in temp_df.index.values:\n",
    "        llr_scores = []\n",
    "        for class_ in [0,1,2]:\n",
    "            num_class_doc = np.sum(labels == class_)\n",
    "            n11 = temp_df.loc[i, class_]\n",
    "            n10 = num_class_doc - n11\n",
    "            n01 = temp_df.loc[i, 'total'] - n11\n",
    "            n00 = (N - n11 - n10 - n01)\n",
    "            pt = (1e-10 + n11 + n01)/N\n",
    "            p1 = n11/(1e-10 + n11 + n10)\n",
    "            p2 = n01/(1e-10 + n01 + n00)\n",
    "\n",
    "\n",
    "            try:\n",
    "                e1 = n11 * (math.log(pt) - math.log(p1))\n",
    "            except:\n",
    "                e1 = 0\n",
    "            try:\n",
    "                e2 = n10 * (math.log(1-pt) - math.log(1-p1))\n",
    "            except:\n",
    "                e2 = 0\n",
    "            try:\n",
    "                e3 = n01 * (math.log(pt) - math.log(p2))\n",
    "            except:\n",
    "                e3 = 0\n",
    "            try:\n",
    "                e4 = n00 * (math.log(1-pt) - math.log(1-p2))\n",
    "            except:\n",
    "                e4 = 0\n",
    "\n",
    "            llr_score = -2 * (e1+e2+e3+e4)\n",
    "            if n11 < n01:\n",
    "                llr_score = 0\n",
    "            llr_scores.append(llr_score)\n",
    "\n",
    "        total_scores.append(llr_scores)\n",
    "    \n",
    "    llr_df = pd.DataFrame(np.array(total_scores), index=temp_df.index, columns=temp_df.columns.values[:-1])\n",
    "\n",
    "    return llr_df\n",
    "\n",
    "\n",
    "def generate_llr_score(data, aspect):\n",
    "    temp = word_class_freq(data, aspect)\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp)\n",
    "    temp_df['total'] = np.sum(temp, -1)\n",
    "    temp_df = temp_df[temp_df['total'] != 0]\n",
    "    temp_df = temp_df.drop(0,0)\n",
    "    \n",
    "    return calculate_llr(temp_df, data[aspect].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Aspects</th>\n",
       "      <th>LEG</th>\n",
       "      <th>SIT</th>\n",
       "      <th>ENT</th>\n",
       "      <th>CUS</th>\n",
       "      <th>VOM</th>\n",
       "      <th>CLE</th>\n",
       "      <th>CKI</th>\n",
       "      <th>FNB</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With everyone trying to get home in the Covid ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ad a lot of people did, we had to scramble to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After coming into Changi airport and worrying ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2044, 2746, 2046, 11132, 2072, 3199, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great service, great plane, great pricing. We ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Legroom:4, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband and I were to fly home from Houston...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190981</th>\n",
       "      <td>We booked to fly from Heathrow to Newark. The ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Legroom:2, Seat comfort:2, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2057, 17414, 2000, 4875, 2013, 9895, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190982</th>\n",
       "      <td>Love Virgin, great staff, food good, quality o...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190983</th>\n",
       "      <td>Virgin upper class is outstanding, really very...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190984</th>\n",
       "      <td>Virgins premium economy is the best I have com...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Legroom:3, Seat comfort:5, In-flight Entertai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 6261, 2015, 12882, 4610, 2003, 1996, 219...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190985</th>\n",
       "      <td>At my age I cannot face an overnight in econom...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Legroom:5, Seat comfort:4, In-flight Entertai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152574 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ReviewText  Rating  \\\n",
       "0       With everyone trying to get home in the Covid ...       4   \n",
       "1       Ad a lot of people did, we had to scramble to ...       5   \n",
       "2       After coming into Changi airport and worrying ...       4   \n",
       "3       Great service, great plane, great pricing. We ...       4   \n",
       "4       My husband and I were to fly home from Houston...       1   \n",
       "...                                                   ...     ...   \n",
       "190981  We booked to fly from Heathrow to Newark. The ...       1   \n",
       "190982  Love Virgin, great staff, food good, quality o...       5   \n",
       "190983  Virgin upper class is outstanding, really very...       5   \n",
       "190984  Virgins premium economy is the best I have com...       5   \n",
       "190985  At my age I cannot face an overnight in econom...       2   \n",
       "\n",
       "                                                  Aspects  LEG  SIT  ENT  CUS  \\\n",
       "0       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "1       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "2       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "3       [Legroom:4, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "4       [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    0   \n",
       "...                                                   ...  ...  ...  ...  ...   \n",
       "190981  [Legroom:2, Seat comfort:2, In-flight Entertai...    0    0    0    0   \n",
       "190982  [Legroom:5, Seat comfort:5, In-flight Entertai...    1    1    1    1   \n",
       "190983  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    1    1   \n",
       "190984  [Legroom:3, Seat comfort:5, In-flight Entertai...    0    1    1    0   \n",
       "190985  [Legroom:5, Seat comfort:4, In-flight Entertai...    1    1    0    0   \n",
       "\n",
       "        VOM  CLE  CKI  FNB                                          input_ids  \\\n",
       "0         1    1    1    1  [101, 2007, 3071, 2667, 2000, 2131, 2188, 1999...   \n",
       "1         1    1    1    1  [101, 4748, 1037, 2843, 1997, 2111, 2106, 1010...   \n",
       "2         1    1    1    1  [101, 2044, 2746, 2046, 11132, 2072, 3199, 199...   \n",
       "3         1    1    1    1  [101, 2307, 2326, 1010, 2307, 4946, 1010, 2307...   \n",
       "4         1    1    1    1  [101, 2026, 3129, 1998, 1045, 2020, 2000, 4875...   \n",
       "...     ...  ...  ...  ...                                                ...   \n",
       "190981    0    2    2    2  [101, 2057, 17414, 2000, 4875, 2013, 9895, 105...   \n",
       "190982    1    2    2    2  [101, 2293, 6261, 1010, 2307, 3095, 1010, 2833...   \n",
       "190983    1    2    2    2  [101, 6261, 3356, 2465, 2003, 5151, 1010, 2428...   \n",
       "190984    0    2    2    2  [101, 6261, 2015, 12882, 4610, 2003, 1996, 219...   \n",
       "190985    0    2    2    2  [101, 2012, 2026, 2287, 1045, 3685, 2227, 2019...   \n",
       "\n",
       "                          labels  \n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4       [1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "...                          ...  \n",
       "190981  [0, 0, 0, 0, 0, 2, 2, 2]  \n",
       "190982  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190983  [1, 1, 1, 1, 1, 2, 2, 2]  \n",
       "190984  [0, 1, 1, 0, 0, 2, 2, 2]  \n",
       "190985  [1, 1, 0, 0, 0, 2, 2, 2]  \n",
       "\n",
       "[152574 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, tokenizer = get_data('./data/data_v3.txt', COL_NAMES)\n",
    "data['labels'] = list(data.iloc[:, 3:11].values)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data.ReviewText.values)\n",
    "labels = np.array([i.tolist() for i in data.labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7ff49d118b4233ba3133211b4ffea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEG,\t40.51,    40.10,    40.27,    66.29\n",
      "SIT,\t41.83,    41.30,    41.52,    67.91\n",
      "ENT,\t44.77,    44.48,    44.60,    58.15\n",
      "CUS,\t45.83,    45.41,    45.61,    81.21\n",
      "VOM,\t42.60,    42.44,    42.50,    72.53\n",
      "CLE,\t39.81,    39.38,    39.57,    58.41\n",
      "CKI,\t40.31,    40.02,    40.16,    57.43\n",
      "FNB,\t39.93,    39.88,    39.90,    46.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e5a778afd949f5a5b67d7cfa5d5df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEG,\t46.93,    35.15,    31.88,    72.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIT,\t48.50,    36.97,    35.20,    73.76\n",
      "ENT,\t51.36,    37.74,    34.42,    63.59\n",
      "CUS,\t57.15,    38.15,    38.94,    84.59\n",
      "VOM,\t52.55,    37.45,    36.71,    78.66\n",
      "CLE,\t48.47,    33.45,    27.63,    69.65\n",
      "CKI,\t62.72,    34.41,    29.31,    68.89\n",
      "FNB,\t51.58,    39.03,    33.79,    56.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a21497779814b1294aae549a16ca077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEG,\t53.44,    39.27,    39.68,    70.66\n",
      "SIT,\t54.27,    40.27,    40.87,    71.25\n",
      "ENT,\t47.86,    42.39,    42.37,    61.61\n",
      "CUS,\t46.87,    45.76,    46.22,    83.33\n",
      "VOM,\t47.08,    42.98,    43.02,    76.36\n",
      "CLE,\t42.08,    39.79,    38.27,    66.16\n",
      "CKI,\t43.32,    43.25,    40.51,    64.59\n",
      "FNB,\t42.70,    41.87,    39.64,    52.43\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10)\n",
    "\n",
    "method_predict = []\n",
    "\n",
    "for method in [DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier()]:\n",
    "    # TRAINING PHASE\n",
    "    last_predict = []\n",
    "    for train_idx, test_idx in tqdm.notebook.tqdm(kf.split(X)):\n",
    "        # Take train and test data\n",
    "        x_train = X[train_idx]\n",
    "        y_train = labels[train_idx]\n",
    "        x_test = X[test_idx]\n",
    "        y_test = labels[test_idx]\n",
    "\n",
    "        # Initate model\n",
    "        clf = method\n",
    "        multi_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "        multi_clf.fit(x_train, y_train)\n",
    "        predicted = multi_clf.predict(x_test).tolist()\n",
    "        last_predict.extend(predicted)\n",
    "    \n",
    "    # SAVE PREDICT\n",
    "    method_predict.append(torch.tensor(last_predict))\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    y_true = labels\n",
    "    y_predict = np.array(torch.tensor(last_predict))\n",
    "\n",
    "    for i, asp in enumerate(ASPECT_NAMES):\n",
    "        print(f'{asp},\\t{precision_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "    {accuracy_score(y_true[:,i], y_predict[:,i])*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(y_predict), './result/RF.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_predict in method_predict:    \n",
    "    for i, asp in enumerate(ASPECT_NAMES):\n",
    "        print(f'{asp}, {precision_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {recall_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {f1_score(y_true[:,i], y_predict[:,i], average=\"macro\")*100:.2f},\\\n",
    "        {accuracy_score(y_true[:,i], y_predict[:,i])*100:.2f}')\n",
    "    print(f'------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
